{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "associate-basis",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install babypandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3a328b",
   "metadata": {},
   "source": [
    "<img src=\"images/banner_friends.jpg\" width=80%>\n",
    "\n",
    "<a name='outline'></a>\n",
    "\n",
    "## Outline of the Project \n",
    "\n",
    "This project has seven sections. Use the outline below to help you quickly navigate to the part of the project you're working on. Questions are worth one point each, unless they contain a ‚≠êÔ∏è‚≠êÔ∏è next to them, in which case they are worth two points (e.g. **Question 1.2. ‚≠êÔ∏è‚≠êÔ∏è**). In most cases, questions worth two points are longer and more challenging than questions worth one point, though sometimes two point questions are themselves straightforward, but used to test the correctness of a previously implemented function.\n",
    "\n",
    "-  [Welcome üëã](#welcome)\n",
    "-  [About the Show üì∫](#about_show)\n",
    "-  [The RebootüÜï](#reboot)\n",
    "-  [About the Data üíæ](#about_data)\n",
    "-  Section 1: [The Pilot üé¨](#section1) \n",
    "-  Section 2: [The One with the Best Director üèÜ](#section2)   \n",
    "-  Section 3: [The One About Gender Balance üë©‚öñÔ∏èüßë](#section3)  \n",
    "-  Section 4: [The Emotional One üòÅüò¢üò®](#section4) \n",
    "-  Section 5: [The One with the Spoilers üôä](#section5) \n",
    "-  Section 6: [The One with the Generated Episode Titles üñ®Ô∏è](#section6) \n",
    "-  Section 7: [The Last One üîöü•≥](#section7) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2465d836",
   "metadata": {},
   "source": [
    "<a id='welcome'></a>\n",
    "## Welcome üëã\n",
    "\n",
    "Welcome to the Final Project! Projects in DSC 10 are similar in format to homeworks, but are different in a few key ways. First, a project is comprehensive, meaning that it draws upon everything we've learned this quarter so far. Second, since problems can vary quite a bit in difficulty, some problems will be worth more points than others. Finally, in a project, the problems are more open-ended; they will usually ask for some result, but won't tell you what method should be used to get it. There might be several equally-valid approaches, and several steps might be necessary. This is closer to how data science is done in \"real life\".\n",
    "\n",
    "It is important that you **start early** on the project! It is the final assignment that is due this quarter, but it is due just a few days before the Final Exam. You are especially encouraged to **find a partner** to work through the project with. You can work with anyone from any section of the course, but you must follow the [Project Partner Guidelines](https://dsc10.com/project-partners/) on the course website. In particular, you are both required to actively contribute to all parts of the project, and you are not allowed to split up the problems and each work on certain problems. If working in a pair, you should submit one notebook to Gradescope for the both of you. \n",
    "\n",
    "**Important:** The `otter` tests don't usually tell you that your answer is correct. More often, they help catch basic mistakes. It's up to you to ensure that your answer is correct. If you're not sure, ask someone (not for the answer, but for some guidance about your approach). Directly sharing answers between groups is not okay, but discussing problems with the course staff or with other students is encouraged.\n",
    "\n",
    "Please do not import any additional packages - you don't need them, and our autograder may not be able to run your code if you do.\n",
    "\n",
    "As you work through this project, there are a few resources you may want to have open:\n",
    "- [`babypandas` Notes](https://notes.dsc10.com/front.html)\n",
    "- [Course Textbook](https://inferentialthinking.com/chapters/intro.html)\n",
    "- [DSC 10 Reference Sheet](https://drive.google.com/file/d/1mQApk9Ovdi-QVqMgnNcq5dZcWucUKoG-/view)\n",
    "- [`babypandas` Documentation](https://babypandas.readthedocs.io/en/latest/)\n",
    "- Other links in the [Resources](https://dsc10.com/resources/) and [Debugging](https://dsc10.com/debugging/) tabs of the course website\n",
    "\n",
    "Start early, good luck, and let's get started! üòé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48ff9808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't change this cell; just run it.\n",
    "import babypandas as bpd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "#import otter\n",
    "#grader = otter.Notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7ba4ec",
   "metadata": {},
   "source": [
    "<a id='about_show'></a>\n",
    "## About the Show üì∫\n",
    "\n",
    "*Friends* is a TV sitcom that aired from 1994 to 2004. The show revolves around six main characters, a group of friends in their 20s and 30s who live in New York City üóΩüåá.  Viewers watch these characters' lives, careers, and relationships develop over a ten year period.\n",
    "\n",
    "<img src=\"images/friends.png\" width=\"500\" height=\"auto\">\n",
    "\n",
    "The show was critically well-received and popular among viewers. It set a number of records including:\n",
    "- the number one television show in 2002\n",
    "- the fifth-most-watched series finale in television history\n",
    "- the most-watched episode of the 2000s decade\n",
    "- Outstanding Comedy Series Primetime Emmy award üèÜ\n",
    "- one of *TV Guide*'s 50 Greatest Shows of All Time\n",
    "\n",
    "<img src=\"images/friends_award.png\" width=\"500\" height=\"auto\">\n",
    "\n",
    "*Friends* used to be available to stream on Netflix, but with its incredible success, the show became too expensive to continue offering access. The last time *Friends* was on Netflix in the US, [it cost Netflix $100 million dollars üí∏](https://slate.com/culture/2018/12/netflix-friends-streaming-100-million.html) just to offer the show on their streaming platform for one year!\n",
    "\n",
    "If you'd like to see the show for yourself, all 10 seasons are available on [MAX](https://www.max.com/shows/friends/52dae4c7-2ab1-4bb9-ab1c-8100fd54e2f9?utm_id=sa%7C71700000110368783%7C58700008392050425%7Cp78367425622&gad_source=1&gclid=CjwKCAiAuYuvBhApEiwAzq_YiRkizSXADw1h1V4etPEJoJ6vam8ozaJ5pO1m9XqTh4JFxCZFO_7skxoC9jIQAvD_BwE&gclsrc=aw.ds). üé•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891541cf",
   "metadata": {},
   "source": [
    "<a id='reboot'></a>\n",
    "## The Reboot üÜï\n",
    "\n",
    "As an avid fan of *Friends*, you have decided to make your own reboot of the classic sitcom. A reboot of a TV show is a new show that takes place in the same setting as an older show, perhaps updated in some ways. Some reboots have a brand new cast with just a few original characters, meanwhile some reboots get the whole cast of the original show to star in the new version. For example, the reboot *How I Met Your Father* is based on the original *How I Met Your Mother*. Other examples include *That 90's Show*, a reboot of *That 70's Show*, and *Fuller House*, a reboot of *Full House*.\n",
    "\n",
    "<img src=\"images/full_house.png\" width=\"500\" height=\"auto\">\n",
    "\n",
    "You've noticed that sitcom reboots are on the rise and you want to get in on the action by finding all the best and most successful aspects of *Friends* and turning them into an even better version. In order to make the reboot, you'll have to do a lot of planning and research to present your plan to the production company that will fund the show. Here's a list of all the things you'll want to have prepared by the time you meet with the producers. Each item on the list represents a section of this project.\n",
    "\n",
    "1. Create a list of writers and directors to contact.\n",
    "2. Select a top choice for the director of the reboot.\n",
    "3. Determine whether to assign spoken lines to male and female characters equally.\n",
    "4. Make a visualization showing the emotional range for each main character to present to the writers.\n",
    "5. Decide whether the main characters should be romantically involved with one another.\n",
    "6. Make a list of episode titles for the first season of the new show.\n",
    "7. Decide how many episodes the reboot should have, based on viewership and rating data.\n",
    "\n",
    "After you've done all of these things you will have completed your plan for the reboot!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60305296",
   "metadata": {},
   "source": [
    "<a id='about_data'></a>\n",
    "## About the Data üíæ\n",
    "\n",
    "Our data, [originally from Kaggle](https://www.kaggle.com/datasets/sujaykapadnis/friends?select=friends.csv), includes several different CSVs about *Friends* episodes, lines, and emotions. If you find any errors in the data, do not attempt to fix them; just analyze the data you are given. \n",
    "\n",
    "### Episodes üé¨\n",
    "\n",
    "The `episodes` DataFrame has a row for each episode from all 10 seasons of *Friends*. For each episode, we have the \n",
    "- season number (`'season'`)\n",
    "- episode number (`'episode'`)\n",
    "- title (`'title'`)\n",
    "- director(s) (`'directed_by'`)\n",
    "- writer(s) (`'written_by'`)\n",
    "- premiere date (`'air_date'`)\n",
    "- number of views on premiere date in millions (`'us_views_millions'`)\n",
    "- rating out of ten (`'imdb_rating'`)\n",
    "\n",
    "For example, the first episode of the first season is called `'The Pilot'`. It was directed by James Burrows and written by David Crane and Marta Kauffman. When it aired on September 22nd, 1994, there were 21.5 million viewers, and it was rated 8.3/10 on IMDb.\n",
    "\n",
    "Run the cell below to load in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e64d4186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>episode</th>\n",
       "      <th>title</th>\n",
       "      <th>directed_by</th>\n",
       "      <th>written_by</th>\n",
       "      <th>air_date</th>\n",
       "      <th>us_views_millions</th>\n",
       "      <th>imdb_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>The Pilot</td>\n",
       "      <td>James Burrows</td>\n",
       "      <td>David Crane &amp; Marta Kauffman</td>\n",
       "      <td>9/22/94</td>\n",
       "      <td>21.50</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>The One with the Sonogram at the End</td>\n",
       "      <td>James Burrows</td>\n",
       "      <td>David Crane &amp; Marta Kauffman</td>\n",
       "      <td>9/29/94</td>\n",
       "      <td>20.20</td>\n",
       "      <td>8.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>The One with the Thumb</td>\n",
       "      <td>James Burrows</td>\n",
       "      <td>Jeffrey Astrof &amp; Mike Sikowitz</td>\n",
       "      <td>10/6/94</td>\n",
       "      <td>19.50</td>\n",
       "      <td>8.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>The One with George Stephanopoulos</td>\n",
       "      <td>James Burrows</td>\n",
       "      <td>Alexa Junge</td>\n",
       "      <td>10/13/94</td>\n",
       "      <td>19.70</td>\n",
       "      <td>8.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>The One with the East German Laundry Detergent</td>\n",
       "      <td>Pamela Fryman</td>\n",
       "      <td>Jeff Greenstein &amp; Jeff Strauss</td>\n",
       "      <td>10/20/94</td>\n",
       "      <td>18.60</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>The One with Princess Consuela</td>\n",
       "      <td>Gary Halvorson</td>\n",
       "      <td>Story by: Robert CarlockTeleplay by: Tracy Reilly</td>\n",
       "      <td>2/26/04</td>\n",
       "      <td>22.83</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>The One Where Estelle Dies</td>\n",
       "      <td>Gary Halvorson</td>\n",
       "      <td>Story by: Mark KunerthTeleplay by: David Crane...</td>\n",
       "      <td>4/22/04</td>\n",
       "      <td>22.64</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>The One with Rachel's Going Away Party</td>\n",
       "      <td>Gary Halvorson</td>\n",
       "      <td>Andrew Reich &amp; Ted Cohen</td>\n",
       "      <td>4/29/04</td>\n",
       "      <td>24.51</td>\n",
       "      <td>8.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>The Last One</td>\n",
       "      <td>Kevin S. Bright</td>\n",
       "      <td>Marta Kauffman &amp; David Crane</td>\n",
       "      <td>5/6/04</td>\n",
       "      <td>52.46</td>\n",
       "      <td>9.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>The Last One</td>\n",
       "      <td>Kevin S. Bright</td>\n",
       "      <td>Marta Kauffman &amp; David Crane</td>\n",
       "      <td>5/6/04</td>\n",
       "      <td>52.46</td>\n",
       "      <td>9.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>236 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     season  episode                                           title  \\\n",
       "0         1        1                                       The Pilot   \n",
       "1         1        2            The One with the Sonogram at the End   \n",
       "2         1        3                          The One with the Thumb   \n",
       "3         1        4              The One with George Stephanopoulos   \n",
       "4         1        5  The One with the East German Laundry Detergent   \n",
       "..      ...      ...                                             ...   \n",
       "231      10       14                  The One with Princess Consuela   \n",
       "232      10       15                      The One Where Estelle Dies   \n",
       "233      10       16          The One with Rachel's Going Away Party   \n",
       "234      10       17                                    The Last One   \n",
       "235      10       18                                    The Last One   \n",
       "\n",
       "         directed_by                                         written_by  \\\n",
       "0      James Burrows                       David Crane & Marta Kauffman   \n",
       "1      James Burrows                       David Crane & Marta Kauffman   \n",
       "2      James Burrows                     Jeffrey Astrof & Mike Sikowitz   \n",
       "3      James Burrows                                        Alexa Junge   \n",
       "4      Pamela Fryman                     Jeff Greenstein & Jeff Strauss   \n",
       "..               ...                                                ...   \n",
       "231   Gary Halvorson  Story by: Robert CarlockTeleplay by: Tracy Reilly   \n",
       "232   Gary Halvorson  Story by: Mark KunerthTeleplay by: David Crane...   \n",
       "233   Gary Halvorson                           Andrew Reich & Ted Cohen   \n",
       "234  Kevin S. Bright                       Marta Kauffman & David Crane   \n",
       "235  Kevin S. Bright                       Marta Kauffman & David Crane   \n",
       "\n",
       "     air_date  us_views_millions  imdb_rating  \n",
       "0     9/22/94              21.50          8.3  \n",
       "1     9/29/94              20.20          8.1  \n",
       "2     10/6/94              19.50          8.2  \n",
       "3    10/13/94              19.70          8.1  \n",
       "4    10/20/94              18.60          8.5  \n",
       "..        ...                ...          ...  \n",
       "231   2/26/04              22.83          8.6  \n",
       "232   4/22/04              22.64          8.5  \n",
       "233   4/29/04              24.51          8.9  \n",
       "234    5/6/04              52.46          9.7  \n",
       "235    5/6/04              52.46          9.7  \n",
       "\n",
       "[236 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episodes = bpd.read_csv('friends_info.csv')\n",
    "episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5238467",
   "metadata": {},
   "source": [
    "### Lines üó£\n",
    "\n",
    "While the `episodes` DataFrame will give you plenty of information on the writers and directors of the show and the success of each episode, you want to know more about the characters and their interactions on the show. You have detailed line-by-line scripts for the dialogue and scene directions of 30 randomly selected *Friends* episodes, stored in a DataFrame called `lines`. For each spoken line or scene direction of these 30 episodes, the `lines` DataFrame includes the following columns:\n",
    "- season number (`'season'`)\n",
    "- episode number (`'episode'`)\n",
    "- scene number (`'scene'`)\n",
    "- line number (`'utterance'`)\n",
    "- character that said the line (`'speaker'`), which is sometimes just `'Scene Directions'`\n",
    "- text of the line (`'text'`)\n",
    "\n",
    "For example, the seventh episode of season one opens with character Rachel Green saying `\n",
    "'Everybody? Shh, shhh. Uhhh... Central Perk is proud to present the music of Miss Phoebe Buffay.'`\n",
    "\n",
    "Run the cell below to load in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b57a1bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>speaker</th>\n",
       "      <th>season</th>\n",
       "      <th>episode</th>\n",
       "      <th>scene</th>\n",
       "      <th>utterance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Everybody? Shh, shhh. Uhhh... Central Perk is ...</td>\n",
       "      <td>Rachel Green</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(applause)</td>\n",
       "      <td>Scene Directions</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi. Um, I want to start with a song thats abou...</td>\n",
       "      <td>Phoebe Buffay</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Oh, great. This is just...</td>\n",
       "      <td>Chandler Bing</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Chandler sees that there is a gorgeous model ...</td>\n",
       "      <td>Scene Directions</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8452</th>\n",
       "      <td>That I can do.</td>\n",
       "      <td>Joey Tribbiani</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8453</th>\n",
       "      <td>Come on! You can drink a gallon of milk in 10 ...</td>\n",
       "      <td>Phoebe Buffay</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8454</th>\n",
       "      <td>All right, watch me! Okay, you time me. Ready?</td>\n",
       "      <td>Joey Tribbiani</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8455</th>\n",
       "      <td>Ready... GO!</td>\n",
       "      <td>Phoebe Buffay</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8456</th>\n",
       "      <td>You did it!</td>\n",
       "      <td>Phoebe Buffay</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8457 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text           speaker  \\\n",
       "0     Everybody? Shh, shhh. Uhhh... Central Perk is ...      Rachel Green   \n",
       "1                                            (applause)  Scene Directions   \n",
       "2     Hi. Um, I want to start with a song thats abou...     Phoebe Buffay   \n",
       "3                            Oh, great. This is just...     Chandler Bing   \n",
       "4     (Chandler sees that there is a gorgeous model ...  Scene Directions   \n",
       "...                                                 ...               ...   \n",
       "8452                                     That I can do.    Joey Tribbiani   \n",
       "8453  Come on! You can drink a gallon of milk in 10 ...     Phoebe Buffay   \n",
       "8454     All right, watch me! Okay, you time me. Ready?    Joey Tribbiani   \n",
       "8455                                       Ready... GO!     Phoebe Buffay   \n",
       "8456                                        You did it!     Phoebe Buffay   \n",
       "\n",
       "      season  episode  scene  utterance  \n",
       "0          1        7      1          1  \n",
       "1          1        7      1          2  \n",
       "2          1        7      1          3  \n",
       "3          1        7      2          1  \n",
       "4          1        7      2          2  \n",
       "...      ...      ...    ...        ...  \n",
       "8452      10       13     13          9  \n",
       "8453      10       13     13         10  \n",
       "8454      10       13     13         11  \n",
       "8455      10       13     13         12  \n",
       "8456      10       13     13         13  \n",
       "\n",
       "[8457 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = bpd.read_csv('friends_sample.csv') \n",
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d06ada0",
   "metadata": {},
   "source": [
    "### Emotions\n",
    "\n",
    "Lastly, you have a dataset of the emotion attached to each line of the show, for most (but not all) lines in the first four seasons of the show. For each line included in our dataset, the line is associated with one of seven different emotions. These seven emotions are:\n",
    "- `'Joyful'` üòÑ\n",
    "- `'Mad'` üò°\n",
    "- `'Sad'` üò≠\n",
    "- `'Powerful'` üí™\n",
    "- `'Peaceful'` ‚úå \n",
    "- `'Scared'` üò± \n",
    "- `'Neutral'` üòë \n",
    "\n",
    "Classifying each line's emotion is a huge data science task in and of itself. [This paper](https://arxiv.org/pdf/1708.04299) explains how convolutional neural networks were able to do this task, if you're interested in learning more.\n",
    "\n",
    "Run the cell below to load in the data, which has the same columns as `lines` plus an extra column called `emotion`. For example, the first line of the fourth scene in Season 1, Episode 1 is categorized as `'Mad'`. That means the speaker, Ross, was probably mad when he said it. If you've ever assembled furniture, you might understand this frustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "609c635f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>speaker</th>\n",
       "      <th>season</th>\n",
       "      <th>episode</th>\n",
       "      <th>scene</th>\n",
       "      <th>utterance</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm supposed to attach a brackety thing to the...</td>\n",
       "      <td>Ross Geller</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Mad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm thinking we've got a bookcase here.</td>\n",
       "      <td>Joey Tribbiani</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's a beautiful thing.</td>\n",
       "      <td>Chandler Bing</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Joyful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What's this?</td>\n",
       "      <td>Joey Tribbiani</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I would have to say that is an 'L'-shaped brac...</td>\n",
       "      <td>Chandler Bing</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12601</th>\n",
       "      <td>Ahh, yes, I will have a glass of the Merlot</td>\n",
       "      <td>Rachel Green</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12602</th>\n",
       "      <td>Okay.</td>\n",
       "      <td>Air Hostess</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12603</th>\n",
       "      <td>And uh, he will have a white wine spritzer.</td>\n",
       "      <td>Rachel Green</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12604</th>\n",
       "      <td>Okay, good. Thank you. I'll be back shortly, a...</td>\n",
       "      <td>Air Hostess</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>Joyful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12605</th>\n",
       "      <td>All right. Woo! Hey, look at that, the airport...</td>\n",
       "      <td>Rachel Green</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>Scared</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12606 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text         speaker  \\\n",
       "0      I'm supposed to attach a brackety thing to the...     Ross Geller   \n",
       "1                I'm thinking we've got a bookcase here.  Joey Tribbiani   \n",
       "2                                It's a beautiful thing.   Chandler Bing   \n",
       "3                                           What's this?  Joey Tribbiani   \n",
       "4      I would have to say that is an 'L'-shaped brac...   Chandler Bing   \n",
       "...                                                  ...             ...   \n",
       "12601        Ahh, yes, I will have a glass of the Merlot    Rachel Green   \n",
       "12602                                              Okay.     Air Hostess   \n",
       "12603        And uh, he will have a white wine spritzer.    Rachel Green   \n",
       "12604  Okay, good. Thank you. I'll be back shortly, a...     Air Hostess   \n",
       "12605  All right. Woo! Hey, look at that, the airport...    Rachel Green   \n",
       "\n",
       "       season  episode  scene  utterance  emotion  \n",
       "0           1        1      4          1      Mad  \n",
       "1           1        1      4          3  Neutral  \n",
       "2           1        1      4          4   Joyful  \n",
       "3           1        1      4          5  Neutral  \n",
       "4           1        1      4          6  Neutral  \n",
       "...       ...      ...    ...        ...      ...  \n",
       "12601       4       24     25          2  Neutral  \n",
       "12602       4       24     25          3  Neutral  \n",
       "12603       4       24     25          4  Neutral  \n",
       "12604       4       24     25          5   Joyful  \n",
       "12605       4       24     25          6   Scared  \n",
       "\n",
       "[12606 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions = bpd.read_csv('friends_emotions.csv') \n",
    "emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cdfc51",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "## Section 1: The Pilot üé¨\n",
    "\n",
    "<small>(<a href=\"#outline\">return to the outline</a>)</small>\n",
    "\n",
    "To start, we'll try to find the most experienced writers and directors in the show to know who to contact for making the reboot. The `episodes` DataFrame contains the writers and directors for each episode, but it's not in the neatest format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0814269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>episode</th>\n",
       "      <th>title</th>\n",
       "      <th>directed_by</th>\n",
       "      <th>written_by</th>\n",
       "      <th>air_date</th>\n",
       "      <th>us_views_millions</th>\n",
       "      <th>imdb_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>The Pilot</td>\n",
       "      <td>James Burrows</td>\n",
       "      <td>David Crane &amp; Marta Kauffman</td>\n",
       "      <td>9/22/94</td>\n",
       "      <td>21.50</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>The One with the Sonogram at the End</td>\n",
       "      <td>James Burrows</td>\n",
       "      <td>David Crane &amp; Marta Kauffman</td>\n",
       "      <td>9/29/94</td>\n",
       "      <td>20.20</td>\n",
       "      <td>8.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>The One with the Thumb</td>\n",
       "      <td>James Burrows</td>\n",
       "      <td>Jeffrey Astrof &amp; Mike Sikowitz</td>\n",
       "      <td>10/6/94</td>\n",
       "      <td>19.50</td>\n",
       "      <td>8.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>The One with George Stephanopoulos</td>\n",
       "      <td>James Burrows</td>\n",
       "      <td>Alexa Junge</td>\n",
       "      <td>10/13/94</td>\n",
       "      <td>19.70</td>\n",
       "      <td>8.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>The One with the East German Laundry Detergent</td>\n",
       "      <td>Pamela Fryman</td>\n",
       "      <td>Jeff Greenstein &amp; Jeff Strauss</td>\n",
       "      <td>10/20/94</td>\n",
       "      <td>18.60</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>The One with Princess Consuela</td>\n",
       "      <td>Gary Halvorson</td>\n",
       "      <td>Story by: Robert CarlockTeleplay by: Tracy Reilly</td>\n",
       "      <td>2/26/04</td>\n",
       "      <td>22.83</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>The One Where Estelle Dies</td>\n",
       "      <td>Gary Halvorson</td>\n",
       "      <td>Story by: Mark KunerthTeleplay by: David Crane...</td>\n",
       "      <td>4/22/04</td>\n",
       "      <td>22.64</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>The One with Rachel's Going Away Party</td>\n",
       "      <td>Gary Halvorson</td>\n",
       "      <td>Andrew Reich &amp; Ted Cohen</td>\n",
       "      <td>4/29/04</td>\n",
       "      <td>24.51</td>\n",
       "      <td>8.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>The Last One</td>\n",
       "      <td>Kevin S. Bright</td>\n",
       "      <td>Marta Kauffman &amp; David Crane</td>\n",
       "      <td>5/6/04</td>\n",
       "      <td>52.46</td>\n",
       "      <td>9.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>The Last One</td>\n",
       "      <td>Kevin S. Bright</td>\n",
       "      <td>Marta Kauffman &amp; David Crane</td>\n",
       "      <td>5/6/04</td>\n",
       "      <td>52.46</td>\n",
       "      <td>9.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>236 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     season  episode                                           title  \\\n",
       "0         1        1                                       The Pilot   \n",
       "1         1        2            The One with the Sonogram at the End   \n",
       "2         1        3                          The One with the Thumb   \n",
       "3         1        4              The One with George Stephanopoulos   \n",
       "4         1        5  The One with the East German Laundry Detergent   \n",
       "..      ...      ...                                             ...   \n",
       "231      10       14                  The One with Princess Consuela   \n",
       "232      10       15                      The One Where Estelle Dies   \n",
       "233      10       16          The One with Rachel's Going Away Party   \n",
       "234      10       17                                    The Last One   \n",
       "235      10       18                                    The Last One   \n",
       "\n",
       "         directed_by                                         written_by  \\\n",
       "0      James Burrows                       David Crane & Marta Kauffman   \n",
       "1      James Burrows                       David Crane & Marta Kauffman   \n",
       "2      James Burrows                     Jeffrey Astrof & Mike Sikowitz   \n",
       "3      James Burrows                                        Alexa Junge   \n",
       "4      Pamela Fryman                     Jeff Greenstein & Jeff Strauss   \n",
       "..               ...                                                ...   \n",
       "231   Gary Halvorson  Story by: Robert CarlockTeleplay by: Tracy Reilly   \n",
       "232   Gary Halvorson  Story by: Mark KunerthTeleplay by: David Crane...   \n",
       "233   Gary Halvorson                           Andrew Reich & Ted Cohen   \n",
       "234  Kevin S. Bright                       Marta Kauffman & David Crane   \n",
       "235  Kevin S. Bright                       Marta Kauffman & David Crane   \n",
       "\n",
       "     air_date  us_views_millions  imdb_rating  \n",
       "0     9/22/94              21.50          8.3  \n",
       "1     9/29/94              20.20          8.1  \n",
       "2     10/6/94              19.50          8.2  \n",
       "3    10/13/94              19.70          8.1  \n",
       "4    10/20/94              18.60          8.5  \n",
       "..        ...                ...          ...  \n",
       "231   2/26/04              22.83          8.6  \n",
       "232   4/22/04              22.64          8.5  \n",
       "233   4/29/04              24.51          8.9  \n",
       "234    5/6/04              52.46          9.7  \n",
       "235    5/6/04              52.46          9.7  \n",
       "\n",
       "[236 rows x 8 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1ce5ab",
   "metadata": {},
   "source": [
    "Some episodes were written or directed by more than one person, and many episodes have certain people who wrote the story and other people who wrote the teleplay. So identifying the most experienced writers and directors is not as straightforward as simply grouping and counting. First, we'll have to clean up the data to break up each string of names into a list of individual names."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a64120",
   "metadata": {},
   "source": [
    "**Question 1.1.**\n",
    "We want to first handle any episodes with two directors so that later we can count the number of episodes each director contributed to. Create a function called `parse_names` that turns a string in the `'directed_by'` column to a **list** of one or two names. Values in the `'directed_by'` column are either single names, or two names separated by an ampersand. For example,\n",
    "-  `parse_names('James Burrows')` should return `['James Burrows']`, and \n",
    "-  `parse_names('Kevin S. Bright & Gary Halvorson')` should return `['Kevin S. Bright', 'Gary Halvorson']`.\n",
    "\n",
    "Then, apply your function to the `'directed_by'` column from the `episodes` DataFrame, and store the resulting Series in the variable `directors_by_episode`. **Do not** modify the `episodes` DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f4b92b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [James Burrows]\n",
       "1        [James Burrows]\n",
       "2        [James Burrows]\n",
       "3        [James Burrows]\n",
       "4        [Pamela Fryman]\n",
       "             ...        \n",
       "231     [Gary Halvorson]\n",
       "232     [Gary Halvorson]\n",
       "233     [Gary Halvorson]\n",
       "234    [Kevin S. Bright]\n",
       "235    [Kevin S. Bright]\n",
       "Name: directed_by, Length: 236, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_names(names):\n",
    "    '''Returns a list of individual names present in the input string.'''\n",
    "    return names.split(\" & \")\n",
    " \n",
    "#parse_names('Kevin S. Bright & Gary Halvorson')   \n",
    "\n",
    "#new_df = episodes.assign(directors_by_episode = episodes.get('directed_by').apply(parse_names))\n",
    "#new_df\n",
    "\n",
    "directors_by_episode = episodes.get('directed_by').apply(parse_names)\n",
    "#new_df.get('directors_by_episode') #parse_names(episodes.get('directed_by'))\n",
    "directors_by_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43edf74e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Kevin S. Bright', 'Gary Halvorson']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_names('Kevin S. Bright & Gary Halvorson')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71f95999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Kevin S. Bright']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directors_by_episode.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6ef2b3e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-48952ef689fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgrader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"q1_1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'grader' is not defined"
     ]
    }
   ],
   "source": [
    "grader.check(\"q1_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "funded-rental",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grader.check(\"q1_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71816a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(episodes.get('directed_by').unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f46f0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes.get('directed_by').unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c51775",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes[episodes.get('directed_by').str.contains('&')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e9c314",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df.get('directors_by_episode').iloc[167]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab597426",
   "metadata": {},
   "outputs": [],
   "source": [
    "directors_by_episode.iloc[167]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e4fedd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(directors_by_episode) #new_df.get('directors_by_episode'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499ea554",
   "metadata": {},
   "source": [
    "\n",
    "For the next question, you'll need to know something interesting about how lists work in Python: when you sum two lists together, the output is one giant list that contains all the elements in both lists combined. An example is shown below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b38d2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "['List', 'combining'] + ['is', 'my', 'passion']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67df10d3",
   "metadata": {},
   "source": [
    "**Question 1.2.** ‚≠ê‚≠ê\n",
    "Now it's time to count the number of episodes each director directed. Make a DataFrame called `directors`, indexed by `'name'`, with one column called `'num_episodes'`. There should be one row for each unique director, and `'num_episodes'` should contain the number of episodes they directed or co-directed. Sort this DataFrame so that the most experienced directors appear at the top.\n",
    "\n",
    "***Hints:***\n",
    "- Our solution involved creating a new, empty DataFrame with `bpd.DataFrame()`, adding columns, and using `groupby`. \n",
    "- For `groupby` to give meaningful results, you must use it on a DataFrame with at least two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d537feb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_directors = directors_by_episode.sum()\n",
    "directors = bpd.DataFrame().assign(name = full_directors, num_episodes = full_directors)\n",
    "directors = directors.groupby('name').count().sort_values(by='num_episodes', ascending=False)\n",
    "directors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc74410",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_directors[167]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82454772",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_directors[168]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32079b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(full_directors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf6c354",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452264a0",
   "metadata": {},
   "source": [
    "Now we want to repeat the process we've done so far in this section, except for writers instead of directors. This is a little trickier because for some episodes, certain writers wrote the story, meaning they developed the plot of the episode, while others wrote the teleplay, which means they wrote the dialogue and scene directions based on the story. For other episodes, the same writers wrote both the story and teleplay.\n",
    "\n",
    "There are two ways writers are represented in the `'written_by'` column:\n",
    "\n",
    "1. One or two names separated by `&` (similar to the `'directed_by'` column). This means these writers wrote both the story and teleplay.\n",
    "2. One or two names separated by `&` after `Story by: ` and one or two names separated by `&` after `Teleplay by: ` \n",
    "\n",
    "Therefore, the six possible templates of strings in the `'written_by'` column are:\n",
    "1. <ins>writer</ins>\n",
    "2. <ins>writer</ins> & <ins>other writer</ins>\n",
    "3. Story by: <ins>writer</ins>Teleplay by: <ins>other writer</ins>\n",
    "4. Story by: <ins>writer</ins>Teleplay by: <ins>other writer</ins> & <ins>another writer</ins>\n",
    "5. Story by: <ins>writer</ins> & <ins>other writer</ins>Teleplay by: <ins>another writer</ins>\n",
    "6. Story by: <ins>writer</ins> & <ins>other writer</ins>Teleplay by: <ins>another writer</ins> & <ins>yet another writer</ins>\n",
    "\n",
    "**Question 1.3.** Implement the function `get_teleplay_writers`, which takes as input a string from the `'written_by'` column, corresponding to one episode, and returns a list of teleplay writers for that episode.  Remember that if the writers' names aren't divided into story and teleplay, then all writers wrote both the story and the teleplay.\n",
    "\n",
    "***Hints:***\n",
    "- Use the `parse_names` function you wrote earlier.\n",
    "- While there are six templates, you can solve this problem without using any conditional logic (`if`-statements). Your solution doesn't have to be the same as ours, but ours was just one line!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7742380",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_names('Kevin S. Bright & Gary Halvorson')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea7fbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_teleplay_writers(names):\n",
    "    '''Returns a list of names of teleplay writers present in the input string.'''\n",
    "    return parse_names(names.split(': ')[-1])\n",
    "    #return parse_names(names) #names.split(\" & \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c1fcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes.get('written_by').iloc[232].split(': ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dba607",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes.get('written_by').iloc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92f6671",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_teleplay_writers(episodes.get('written_by').iloc[4])# == ['Jeff Greenstein', 'Jeff Strauss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153620a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes.get('written_by').iloc[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b1fc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_teleplay_writers(episodes.get('written_by').iloc[15])# == ['Marta Kauffman', 'David Crane']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a9723d",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes.get('written_by').iloc[232]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d06469",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_teleplay_writers(episodes.get('written_by').iloc[232]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d170eb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_teleplay = 'Story by: Mark Kunerth & Jeff GreensteinTeleplay by: David Crane & Marta Kauffman & Test'\n",
    "get_teleplay_writers(test_teleplay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fa1b14",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410e094e",
   "metadata": {},
   "source": [
    "**Question 1.4.** Now we need a function for getting the names of the writers that wrote the story of an episode. Create a function `get_story_writers` that takes a string from the `'written_by'` column and returns a list of story writers. As before, if the writers' names aren't divided into story and teleplay, then all writers wrote both the story and the teleplay.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da01e29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_story_writers(names):\n",
    "    '''Returns a list of names of story writers present in the input string.'''\n",
    "    split_name = names.split(':')#parse_names(names.split(':')[0:])\n",
    "    if len(split_name) == 1:\n",
    "        return parse_names(split_name[0])\n",
    "    else:\n",
    "        writers_list = []\n",
    "        get_story_writers = split_name[1].strip(' ')\n",
    "        parsed_story_writers = parse_names(get_story_writers)\n",
    "        if len(parsed_story_writers) == 1:\n",
    "            parsed_story_writers[0][0:-11]\n",
    "            writers_list = np.append(writers_list, parsed_story_writers[0][0:-11])\n",
    "            return writers_list.tolist()\n",
    "        else:\n",
    "            first = parsed_story_writers[0]\n",
    "            second = parsed_story_writers[1][0:-11]\n",
    "            writers_list = np.append(writers_list, first)\n",
    "            writers_list = np.append(writers_list, second)\n",
    "        #remove_teleplay = parsed_story_writers[1]#[0:-11]\n",
    "#         parse_story_writers = parse_names(get_story_writers)\n",
    "#         parsed_names = parse_names(split_teleplay)\n",
    "            return writers_list.tolist() #get_story_writers#parse_story_writers #.split(': ')#parse_names(names.split(': ')[-1])\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "#     split_teleplay = names.replace('Teleplay', '')\n",
    "#     parse_split_teleplay = parse_names(split_teleplay)\n",
    "#     #split_by_sep = parse_split_teleplay.split(':')#.strip(' ')\n",
    "#     return parse_split_teleplay\n",
    "# #     if len(split_by_sep) == 1:\n",
    "# #         return parse_names(split_by_sep)\n",
    "# #     else:\n",
    "# #         get_story_writers = split_by_sep[1].strip(' ')\n",
    "# #         parse_story_writers = parse_names(get_story_writers)\n",
    "# #         parsed_names = parse_names(split_teleplay)\n",
    "# #         return parse_story_writers #.split(': ')#parse_names(names.split(': ')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b962e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes.get('written_by').iloc[22].split(':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a694124",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes.get('written_by').iloc[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d2455d",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_story_writers(episodes.get('written_by').iloc[22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567a96e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes.get('written_by').iloc[40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948e1d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_story_writers(episodes.get('written_by').iloc[40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee7841d",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes.get('written_by').iloc[232].split(':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be43dbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes.get('written_by').iloc[232]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6dcb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_story_writers(episodes.get('written_by').iloc[232])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e7bf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes.get('written_by').iloc[235]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9233126f",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_story_writers(episodes.get('written_by').iloc[235])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8535e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_story = 'Story by: Mark Kunerth & Baby PandasTeleplay by: David Crane & Marta Kauffman & Test'\n",
    "get_story_writers(test_story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26bea9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_story2 = 'Story by: David Crane & Marta KauffmanTeleplay by: Jeff Greenstein & Jeff Strauss'\n",
    "get_story_writers(test_story2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5999f7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bd465b",
   "metadata": {},
   "source": [
    "**Question 1.5.**\n",
    "Next we want to count how many times each writer wrote or co-wrote the story for an episode, as well as how many times each writer wrote or co-wrote the teleplay for an episode. This will be similar to the process we did in Question 1.2 for directors. Since we want to do something similar for story writers and teleplay writers, let's create a more general function that works for directors, story writers, and teleplay writers.\n",
    "\n",
    "The function `count_episodes` should take as input a Series of lists, containing for each episode, either the directors, story writers, or teleplay writers for that episode. The function should return a DataFrame indexed by `'name'`, with one column called `'num_episodes'` containing a count of how many episodes each staff member worked on, sorted in descending order of the number of episodes.\n",
    "\n",
    "For example `count_episodes(directors_by_episode)` should return a DataFrame that is identical to the `directors` DataFrame you created in Question 1.2. But we could also use the same function, with different inputs, to count the number of episodes each story writer and each teleplay writer worked on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4e086a",
   "metadata": {},
   "outputs": [],
   "source": [
    "directors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677822a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "directors_by_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf77a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "story_writers_by_episode = episodes.get('written_by').apply(get_story_writers)\n",
    "story_writers_by_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc3b93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "teleplay_writers_by_episode = episodes.get('written_by').apply(get_teleplay_writers)\n",
    "teleplay_writers_by_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf28cab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "story_writers_by_episode==teleplay_writers_by_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd8a075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_episodes(staff):\n",
    "    '''Returns a DataFrame showing how many episodes each staff member worked on.''' \n",
    "    full_staff = staff.sum()\n",
    "    staff_df = bpd.DataFrame().assign(name = full_staff, num_episodes = full_staff)\n",
    "    staff_df = staff_df.groupby('name').count().sort_values(by='num_episodes', ascending=False)\n",
    "    return staff_df\n",
    "\n",
    "# An example call to your function. Feel free to change this and try out other inputs.  \n",
    "count_episodes(directors_by_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e787b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_episodes(teleplay_writers_by_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2893afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_episodes(story_writers_by_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afbbe75",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1_5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e755b4db",
   "metadata": {},
   "source": [
    "**Question 1.6.**\n",
    "Now, use the function you wrote in Question 1.5 to make a DataFrame called `story_writers`. This DataFrame should be indexed by `'name'`, with one column called `'num_episodes'`. There should be one row for each unique story writer, and `'num_episodes'` should contain the number of episodes for which they wrote the story, either independently or with a co-writer. Sort this DataFrame so that the most experienced story writers appear at the top.\n",
    "\n",
    "Then do the same for teleplay writers, storing your result in `teleplay_writers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bcf676",
   "metadata": {},
   "outputs": [],
   "source": [
    "story_writers = count_episodes(story_writers_by_episode)\n",
    "story_writers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3112b81e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1_6_a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191bf3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "teleplay_writers= count_episodes(teleplay_writers_by_episode)\n",
    "teleplay_writers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ef0239",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1_6_b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da874745",
   "metadata": {},
   "source": [
    "**Question 1.7.** ‚≠ê‚≠ê\n",
    "Finally, it's time to use the information in `directors`, `story_writers`, and `teleplay_writers` to find the best candidates to help create the reboot. Create an array called `experienced_directors` containing the names of directors that directed at least 20 episodes. Then create an array called `experienced_writers` containing the names of writers that wrote at least 20 stories **and** at least 20 teleplays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655162d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "experienced_directors_df = directors[directors.get('num_episodes')>=20].reset_index()\n",
    "experienced_directors_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f33b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "experienced_story_writers = story_writers[story_writers.get('num_episodes')>=20].reset_index()\n",
    "experienced_story_writers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e107ad8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "experienced_teleplay_writers = teleplay_writers[teleplay_writers.get('num_episodes')>=20].reset_index()\n",
    "experienced_teleplay_writers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee37aa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "experienced_writers_df = experienced_story_writers.merge(experienced_teleplay_writers, on='name')\n",
    "experienced_writers_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7513d3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "experienced_directors = experienced_directors_df.get('name').to_numpy()\n",
    "experienced_writers = experienced_writers_df.get('name').to_numpy()\n",
    "\n",
    "print(f'The experienced directors are {experienced_directors}')\n",
    "print(f'The experienced writers are {experienced_writers}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42b634e",
   "metadata": {},
   "outputs": [],
   "source": [
    "experienced_directors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cd156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(experienced_directors, np.ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69baad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(experienced_writers, np.ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902e8717",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1_7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891cd5dc",
   "metadata": {},
   "source": [
    "These are the writers and directors you'll reach out to about your reboot, and hopefully, you can find at least one writer and one director interested in working with you on the reboot! If not, you can always relax your criteria and ask some less-experienced writers and directors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe1a288",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "## Section 2: The One with the Best Director üèÜ\n",
    "\n",
    "<small>(<a href=\"#outline\">return to the outline</a>)</small>\n",
    "\n",
    "You discovered in Section 1 that the two directors who directed the most episodes were Gary Halvorson and Kevin S. Bright. In this section, we want to discover which of these two directors makes better episodes, as determined by viewers. We'll use the `'imdb_rating'` column from the `episodes` DataFrame to gauge how much people enjoyed an episode. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006d8987",
   "metadata": {},
   "source": [
    "**Question 2.1.** Create a DataFrame `mean_by_director` with the director's name as the index and just one column called `'mean_rating'` that contains the mean rating of all of the episodes directed by that director.\n",
    "\n",
    "***Note:*** There is one episode that had two directors, separated by `'&'`. Exclude this episode, so that we're only looking at episodes with a single director."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eae13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96d8172",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_sep = episodes[episodes.get('directed_by').str.contains('&')]\n",
    "has_sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6052081c",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes_hasnt_sep = episodes[~episodes.get('directed_by').str.contains('&')]\n",
    "episodes_hasnt_sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e5b402",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes.iloc[166:169]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750d797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes_hasnt_sep.iloc[166:169]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f400fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_by_name = episodes_hasnt_sep.assign(name = episodes_hasnt_sep.get('directed_by')).groupby('name').mean()\n",
    "mean_by_director = indexed_by_name.assign(mean_rating = indexed_by_name.get('imdb_rating')).get(['mean_rating'])\n",
    "mean_by_director"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7af1e2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5942a1b",
   "metadata": {},
   "source": [
    "Now let's look at how Gary Halvorson's episodes are rated, on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d1534b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_by_director.get('mean_rating').loc['Gary Halvorson']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3a0e9e",
   "metadata": {},
   "source": [
    "How accurate is this number in reflecting Gary's true potential as a director? In this section, we'll consider the data we have to be a sample of each director's potential abilities. Based on this sample, we want to infer each director's ability more broadly. \n",
    "\n",
    "For example, the average rating of episodes that Gary actually directed was 8.4, but the set of episodes he *might* have directed with additional opportunities (representing his potential as a director) could have had a higher or lower average rating. By looking at a data set of *Friends* episodes directed by Gary, we are only looking at a sample from a larger population.\n",
    "\n",
    "We'll estimate each director's mean episode rating in the population based on their mean episode in the sample using **bootstrapping**.\n",
    "\n",
    "**Question 2.2.** Below, write a function called `simulate_estimates`. It should take 3 arguments:\n",
    "- `director_df`: A DataFrame with a row for each episode, which includes columns `'directed_by'` and `'imdb_rating'`.\n",
    "- `director`: The name of the director whose mean episode rating we are trying to estimate.\n",
    "- `repetitions`: The number of repetitions to perform (the number of resamples to create).\n",
    "\n",
    "It should take `repetitions` resamples with replacement from the rows of `director_df` that correspond to the given `director`. For each of those resamples, it should compute the mean episode rating for that resample. Then it should return an array containing the values of those means for each resample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3e0ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c348a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_estimates(director_df, director, repetitions):\n",
    "    '''Returns an array of length repetitions, \n",
    "    containing bootstrapped means of the data \n",
    "    in the imbdb_rating column for the given director. '''\n",
    "    means_array=np.array([])\n",
    "    for i in np.arange(repetitions):\n",
    "        resample = director_df[director_df.get('directed_by')==director].sample(director_df[director_df.get('directed_by')==director].shape[0], replace=True)\n",
    "        get_means = resample.get('imdb_rating').mean()\n",
    "        means_array = np.append(means_array, get_means)\n",
    "    return means_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcc7ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gary_output = simulate_estimates(episodes, 'Gary Halvorson', 100)\n",
    "gary_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c6d3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(gary_output, np.ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5db1c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gary_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6859853d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83f5833",
   "metadata": {},
   "source": [
    "**Question 2.3.** Use your function `simulate_estimates` to estimate the mean rating of Gary Halvorson's episodes. Use `repetitions = 5000`, and save your array of bootstrapped means in the variable `gary_boot_means`. Repeat the same process for Kevin S. Bright, storing your results in `kevin_boot_means`.  \n",
    "\n",
    "Then, plot the distributions of these arrays in one [overlaid density histogram](https://dsc10.com/resources/lectures/lec07/lec07.html#Plotting-overlaid-histograms). Use the optional parameter `alpha=0.5` in your call to `.plot`, which changes the opacity so you can see the distributions more clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8659f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "gary_boot_means = simulate_estimates(episodes, 'Gary Halvorson', 5000)\n",
    "kevin_boot_means = simulate_estimates(episodes, 'Kevin S. Bright', 5000)\n",
    "\n",
    "most_directed_df = bpd.DataFrame()\n",
    "most_directed_df = most_directed_df.assign(Gary_means = gary_boot_means)\n",
    "most_directed_df = most_directed_df.assign(Kevin_means = kevin_boot_means)\n",
    "most_directed_df\n",
    "\n",
    "# Plot your overlaid histogram here.\n",
    "#fig, ax = plt.subplots(figsize=(10, 5))\n",
    "#bins=np.arange(10_000, 300_000, 10_000)\n",
    "most_directed_df.plot(kind='hist', density=True, alpha=0.5);#, bins=bins, ec='w')\n",
    "#kevin_boot_means.plot(kind='hist', y='Mean', ax=ax, density=True, alpha=0.5, bins=bins, ec='w')\n",
    "#plt.legend([\"Gary's mean\", \"Kevin's mean\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0023222e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81117631",
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(gary_boot_means, np.ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5c0636",
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(kevin_boot_means, np.ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026aad8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gary_boot_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366382f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(kevin_boot_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708fe7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#isinstance(gary_boot_means, np.ndarray) and isinstance(kevin_boot_means, np.ndarray) and len(gary_boot_means) == len(kevin_boot_means) == 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fb8dfe",
   "metadata": {},
   "source": [
    "**Question 2.4.** Now we want to calculate a 99% confidence interval for the mean episode rating of each of the two directors. To do this, create a function `confidence_interval_99`, which takes in an array of bootstrapped statistics `boot_stats` and returns a list of length two, containing the left endpoint and the right endpoint of the 99% confidence interval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f338e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_interval_99(boot_stats):\n",
    "    '''Returns a list of the endpoints of a 99% confidence interval based on boot_stats.'''\n",
    "    bounds = []\n",
    "    lower_bound = np.percentile(boot_stats, 0.5)\n",
    "    upper_bound = np.percentile(boot_stats, 99.5)\n",
    "    bounds = np.append(bounds, lower_bound)\n",
    "    bounds = np.append(bounds, upper_bound)\n",
    "    bounds = bounds.tolist()\n",
    "    return bounds\n",
    "\n",
    "print('Gary 99% CI:', confidence_interval_99(gary_boot_means))\n",
    "print('Kevin 99% CI:', confidence_interval_99(kevin_boot_means))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4afb7c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3924ce",
   "metadata": {},
   "source": [
    "From what we've done so far, we've established that Kevin's episodes are generally rated better than Gary's episodes, however there is some overlap in their confidence intervals. This means it could be the case that Gary is just as strong a director as Kevin, but this is just not reflected in our sample. \n",
    "\n",
    "We'll address this possibility by performing a hypothesis test with the following hypotheses:\n",
    "\n",
    "- **Null Hypothesis:** The mean rating of Kevin's episodes in the population equals the mean rating of Gary's episodes in the population. Equivalently, the difference in the mean rating for Kevin's and Gary's episodes in the population equals 0.\n",
    "- **Alternative Hypothesis:** The mean rating of Kevin's episodes in the population does not equal the mean rating of Gary's episodes in the population. Equivalently, the difference in the mean rating for Kevin's and Gary's episodes in the population does not equal 0.\n",
    "\n",
    "Remember, the population represents all episodes they *might* have directed, or their potential as a director.\n",
    "\n",
    "Since we were able to set up our hypothesis test as a question of whether our population parameter ‚Äì the difference in mean rating for Kevin's and Gary's episodes ‚Äì is equal to a certain value, we can **test our hypotheses by constructing a confidence interval for the parameter**. This is the method we used in [Lecture 22](https://dsc10.com/resources/lectures/lec22/lec22.html#Visualizing-the-distribution-of-each-group) to test whether the mean human body temperature was actually 98.6 degrees Fahrenheit. For a refresher on this method, you can read more about conducting a hypothesis test with a confidence interval in [CIT 13.4](https://inferentialthinking.com/chapters/13/4/Using_Confidence_Intervals.html#)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2703d3c",
   "metadata": {},
   "source": [
    "**Question 2.5.** ‚≠ê‚≠ê Compute 1000 bootstrapped estimates for the difference in the mean rating for Kevin's episodes and Gary's episodes in the population (subtract in the order Kevin minus Gary). Store your 1000 estimates in the `difference_means` array.\n",
    "\n",
    "You should generate your resamples of Kevin's episodes by sampling from the set of episodes directed by Kevin, and similarly for Gary, by sampling from the set of episodes directed by Gary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1553ac74",
   "metadata": {},
   "outputs": [],
   "source": [
    "directors_observed = mean_by_director.loc['Kevin S. Bright'].iloc[0] - mean_by_director.loc['Gary Halvorson'].iloc[0]\n",
    "directors_observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d10f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "difference_means = np.array([])\n",
    "# for i in range(1000):\n",
    "    \n",
    "gary_boot_means_for_diff = simulate_estimates(episodes, 'Gary Halvorson', 1000)\n",
    "kevin_boot_means_for_diff = simulate_estimates(episodes, 'Kevin S. Bright', 1000)\n",
    "\n",
    "difference_means = np.append(difference_means, kevin_boot_means_for_diff - gary_boot_means_for_diff)\n",
    "\n",
    "# # Just display the first ten differences.\n",
    "difference_means[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af1dbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gary_boot_means_for_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e04a2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "kevin_boot_means_for_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74054b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "kevin_boot_means_for_diff-gary_boot_means_for_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4a5d9c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5b9f67",
   "metadata": {},
   "source": [
    "Let's visualize your estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0f679b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(bpd.DataFrame().assign(DifferenceMeans = difference_means)\n",
    " .plot(kind='hist', density=True, ec='w', figsize=(10, 5)));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f887a47",
   "metadata": {},
   "source": [
    "**Question 2.6.** Use the function `confidence_interval_99` you created before to compute a 99% confidence interval for the difference in the mean rating of Kevin's and Gary's episodes (as before, Kevin minus Gary). Assign to `kevin_gary_difference_CI` a list containing the endpoints of this confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67763335",
   "metadata": {},
   "outputs": [],
   "source": [
    "kevin_gary_difference_CI = confidence_interval_99(kevin_boot_means_for_diff - gary_boot_means_for_diff)\n",
    "kevin_gary_difference_CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ecf63c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2308e33",
   "metadata": {},
   "source": [
    "Recall the hypotheses we were testing:\n",
    "\n",
    "- **Null Hypothesis:** The mean rating of Kevin's episodes in the population equals the mean rating of Gary's episodes in the population. Equivalently, the difference in the mean rating for Kevin's and Gary's episodes in the population equals 0.\n",
    "- **Alternative Hypothesis:** The mean rating of Kevin's episodes in the population does not equal the mean rating of Gary's episodes in the population. Equivalently, the difference in the mean rating for Kevin's and Gary's episodes in the population does not equal 0.\n",
    "\n",
    "**Question 2.7.** Based on the confidence interval you've created, would you reject the null hypothesis at the 0.01 significance level? Set `reject_kevin_gary` to True if you would reject the null hypothesis, and False if you would not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb417c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reject_kevin_gary = True #### 0 is not in the CI\n",
    "reject_kevin_gary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd638ef",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a9ffbc",
   "metadata": {},
   "source": [
    "We have now discovered which of these two directors would likely make better episodes for your reboot. However, we also want to know whether Gary and Kevin's episodes have other differences besides the ratings they generate. For example, does one of them tend to direct episodes with more views?\n",
    "\n",
    "Let's write a single function that works for both rating and viewership. To do this, we want to generalize our simulation code from Question 2.2 so that we can create a confidence interval for either variable.\n",
    "\n",
    "**Question 2.8.** Create a function called `compare_gary_kevin`, which takes in 3 inputs:\n",
    "- `sample_df`, a DataFrame with a row for each episode in our sample, which includes a column called `'directed_by'`.\n",
    "- `variable`, the column name of the relevant variable whose mean we want to estimate. \n",
    "- `repetitions`, the number of repetitions to perform (the number of resamples to create).\n",
    "                       \n",
    "The function should adhere to these specifications:\n",
    "1. The function should generate an overlaid density histogram, showing the distributions of bootstrapped means of the given variable, for both Kevin and Gary. Make sure to give your histogram a descriptive title and to use appropriate labels. Use `bins=20`, `alpha=0.5`, and `figsize=(10,5)`.\n",
    "2. The function should print a statement with the 99% confidence interval for the mean value of the given variable in the population, for both Kevin and Gary. See the example below for the type of statement to print, but the exact formatting is up to you.\n",
    "3. The function should return nothing.\n",
    "\n",
    "***Hints:***\n",
    "- This is designed to be a challenging question, but remember that you can use any of the functions you've already created. \n",
    "- Our solution does the necessary operations once for Gary and once for Kevin and assigns columns named `'Gary_mean_estimate'` and `'Kevin_mean_estimate'`.\n",
    "\n",
    "Here is an example output that shows a comparison of estimates for Gary and Kevin's mean `'imdb_rating'`.\n",
    "<img src=\"images/gary_kevin_example.jpg\" width = 700>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e911cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_gary_kevin(sample_df, variable, repetitions):\n",
    "    '''For each of Gary and Kevin, display a distribution of bootstrapped means and\n",
    "    a confidence interval for the mean value of the variable from sample_df.'''\n",
    "    means_array_gary=np.array([])\n",
    "    means_array_kevin=np.array([])\n",
    "\n",
    "    for i in np.arange(repetitions):\n",
    "        resample_gary = sample_df[sample_df.get('directed_by')=='Gary Halvorson'].sample(sample_df[sample_df.get('directed_by')=='Gary Halvorson'].shape[0], replace=True)\n",
    "        get_means_gary = resample_gary.get(variable).mean()\n",
    "        means_array_gary = np.append(means_array_gary, get_means_gary)\n",
    "        \n",
    "        resample_kevin = sample_df[sample_df.get('directed_by')=='Kevin S. Bright'].sample(sample_df[sample_df.get('directed_by')=='Kevin S. Bright'].shape[0], replace=True)\n",
    "        get_means_kevin = resample_kevin.get(variable).mean()\n",
    "        means_array_kevin = np.append(means_array_kevin, get_means_kevin)\n",
    "        \n",
    "    most_directed_df_compare = bpd.DataFrame()\n",
    "    most_directed_df_compare = most_directed_df_compare.assign(Gary_mean_estimate = means_array_gary)\n",
    "    most_directed_df_compare = most_directed_df_compare.assign(Kevin_mean_estimate = means_array_kevin)\n",
    "    most_directed_df_compare\n",
    "    \n",
    "    #fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    #bins=np.arange(10_000, 300_000, 10_000)\n",
    "    most_directed_df_compare.plot(kind='hist', density=True, alpha=0.5, bins=20, ec='w', title=variable);\n",
    "    \n",
    "    print(f\"Gary's 99% CI for {variable}:\", confidence_interval_99(means_array_gary))\n",
    "    print(f\"Kevin's 99% CI for {variable}:\", confidence_interval_99(means_array_kevin))\n",
    "\n",
    "\n",
    "# Try to replicate the graph shown in the example.\n",
    "compare_gary_kevin(episodes, 'imdb_rating', 1000)\n",
    "\n",
    "\n",
    "# gary_boot_means = simulate_estimates(episodes, 'Gary Halvorson', 1000)\n",
    "# kevin_boot_means = simulate_estimates(episodes, 'Kevin S. Bright', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b0c810",
   "metadata": {},
   "source": [
    "**Question 2.9.** ‚≠ê‚≠ê Using the `compare_gary_kevin` function you just wrote, create a plot and confidence intervals that would help you answer the following question.\n",
    "\n",
    ">Whose episodes have more views on average, Kevin's or Gary's?\n",
    "\n",
    "This question is not difficult; it is worth two points because it tests your implementation of the function `compare_gary_kevin` which you wrote in the previous question. All you need to do here is make one call to your function with the appropriate inputs. Use `repetitions=1000`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9807a709",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2_9\n",
    "manual: True\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ccadaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make your function call here. Be sure to run this cell before submitting.\n",
    "compare_gary_kevin(episodes, 'us_views_millions', 1000)\n",
    "\n",
    "# us_views_millions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e3fa06",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "At this point it should be clear which director you'll be reaching out to for your reboot!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb6b962",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "## Section 3: The One About Gender Balance üë©‚öñÔ∏èüßë\n",
    "\n",
    "<small>(<a href=\"#outline\">return to the outline</a>)</small>\n",
    "\n",
    "After watching a couple of episodes of <i>Friends</i>, you start to wonder if the three male main characters (`'Ross Geller'`, \n",
    "`'Chandler Bing'`, `'Joey Tribbiani'`) speak more lines than the the three female main characters (`'Rachel Green'`, `'Monica Geller'`, `'Phoebe Buffay'`). You want your reboot to be true to the spirit of the original show, but you don't feel great about creating a television show that has a significant gender imbalance.\n",
    "\n",
    "Below is the `lines` DataFrame, which contains information on all the lines from a random sample of 30 *Friends* episodes. Notice that `lines` contains **every** line from each one of these 30 episodes, including lines from characters other than the main six, and even the scene directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc52a048",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc1fb83",
   "metadata": {},
   "source": [
    "**Question 3.1.** Write a function named `main_char` that takes in the name of a single speaker and determines whether that speaker is one of the six main characters:\n",
    "- `'Ross Geller'`\n",
    "- `'Chandler Bing'`\n",
    "- `'Joey Tribbiani'`\n",
    "- `'Rachel Green'`\n",
    "- `'Monica Geller'`\n",
    "- `'Phoebe Buffay'`\n",
    "\n",
    "For example, `main_char('Chandler Bing')` should return `True`, but `main_char('Janice Hosenstein')` should return `False`.\n",
    "\n",
    "Then, use your function to filter the `lines` DataFrame to contain only lines spoken by one of the six main characters, saving the result as `main_lines`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd43d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_char(speaker): \n",
    "    '''Returns True if speaker is a main character, False otherwise.'''\n",
    "    if speaker == 'Ross Geller':\n",
    "        return True\n",
    "    elif speaker == 'Chandler Bing':\n",
    "        return True\n",
    "    elif speaker == 'Joey Tribbiani':\n",
    "        return True\n",
    "    elif speaker == 'Rachel Green':\n",
    "        return True\n",
    "    elif speaker == 'Monica Geller':\n",
    "        return True\n",
    "    elif speaker == 'Phoebe Buffay':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "#main_char('bilbo')\n",
    "\n",
    "main_lines = lines[lines.get('speaker').apply(main_char)]\n",
    "main_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5d88ef",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d4d303",
   "metadata": {},
   "source": [
    "**Question 3.2.** Using the `main_lines` DataFrame, count the total number of lines spoken by a main character across all episodes in the sample and assign your answer to the variable `line_count`.\n",
    "\n",
    "Then compute the proportion of these lines that were spoken by male characters (`'Ross Geller'`, \n",
    "`'Chandler Bing'`, `'Joey Tribbiani'`) and the proportion of lines that were spoken by female characters (`'Rachel Green'`, `'Monica Geller'`, `'Phoebe Buffay'`). Assign your answers to the variables `observed_male_prop` and  `observed_female_prop`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07452ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_count = main_lines.shape[0]\n",
    "\n",
    "female_lines = main_lines[(main_lines.get('speaker') == 'Rachel Green') | (main_lines.get('speaker') == 'Monica Geller') | (main_lines.get('speaker') == 'Phoebe Buffay')]\n",
    "count_female_lines = female_lines.shape[0]\n",
    "\n",
    "male_lines = main_lines[(main_lines.get('speaker') == 'Ross Geller') | (main_lines.get('speaker') == 'Chandler Bing') | (main_lines.get('speaker') == 'Joey Tribbiani')]\n",
    "count_male_lines = male_lines.shape[0]\n",
    "\n",
    "observed_female_prop = count_female_lines/line_count\n",
    "observed_male_prop = count_male_lines/line_count\n",
    "\n",
    "print('Number of Lines: ' + str(line_count))\n",
    "print('Male Proportion: ' + str(observed_male_prop))\n",
    "print('Female Proportion: ' + str(observed_female_prop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feba6be4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e52dc6",
   "metadata": {},
   "source": [
    "You recognize that `observed_female_prop` and `observed_male_prop` are similar but they're not exactly the same. Is this just random chance that your sample happened to include more lines by male main characters? Or is the case that throughout the show *Friends*, lines spoken by one of the six main characters are actually more likely to be spoken by male characters? Let's do a hypothesis test with the following hypotheses:\n",
    "\n",
    "- **Null Hypothesis**: Throughout the ten seasons of *Friends*, male main characters speak the same number of lines as female main characters.  \n",
    "- **Alternative Hypothesis**: Throughout the ten seasons of *Friends*, male main characters speak more lines than female main characters.\n",
    "\n",
    "Run the cell below to define a variable `null_distribution` that shows the proportion of each gender according to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06ca299",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_distribution = np.array([0.5, 0.5])\n",
    "null_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3e2aab",
   "metadata": {},
   "source": [
    "**Question 3.3.** To perform our hypothesis test, we will simulate drawing a random sample of size `line_count` from the null distribution, and then compute a test statistic on each simulated sample. We must first choose a reasonable test statistic that will help us determine whether or not to reject the null hypothesis.\n",
    "\n",
    "From the options below, find **all** valid test statistics that we could use for this hypothesis test. Save the numbers of your choices in a **list** called `gender_test_statistics`. Valid test statistics are ones that would allow us to distinguish between the null and alternative hypotheses. \n",
    "\n",
    "***Hint:*** To determine whether a test statistic is valid, think about which values of the statistic (high, low, moderate) would make you lean towards the null and which would make you lean towards the alternative.\n",
    "\n",
    "1. The difference between the proportion of female-spoken lines and 0.5.\n",
    "2. The difference between the number of male-spoken lines and the number of female-spoken lines. \n",
    "3. The difference between the number of female-spoken lines and one half of `line_count`.\n",
    "4. Three times the difference between the proportion of male-spoken lines and 0.5.\n",
    "5. The total variation distance between the gender distribution of lines spoken and the null distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb956bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_test_statistics = [1, 2, 3, 4]\n",
    "gender_test_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb1d372",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a73e31c",
   "metadata": {},
   "source": [
    "**Question 3.4.** For this hypothesis test, we'll use the proportion of male-spoken lines as our test statistic. Write a simulation that runs 10,000 times, each time drawing a random sample of size `line_count` under the assumption of the null hypothesis. Keep track of the simulated test statistics in the `gender_stats` array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc1dd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a236152",
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_male_prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0de476",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_stats = np.array([])\n",
    "for i in np.arange(10000):\n",
    "    sampling = np.random.multinomial(line_count, null_distribution)\n",
    "    proportioning = sampling[0] / line_count\n",
    "    gender_stats = np.append(gender_stats, proportioning)\n",
    "\n",
    "\n",
    "###Visualize with a histogram\n",
    "bpd.DataFrame().assign(gender_stats=gender_stats).plot(kind='hist', density=True, ec='w', figsize=(10, 5));\n",
    "plt.axvline(x=observed_male_prop, color='black', linewidth=4, label='observed statistic')\n",
    "plt.legend();\n",
    "gender_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7495a3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = np.array([])\n",
    "# for i in np.arange(10000):\n",
    "#     result =  sample_proportions(line_count, eligible_population)\n",
    "#     results = np.append(results, result)\n",
    "    \n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079662d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# results = np.array([])\n",
    "# for i in np.arange(10000):\n",
    "#     result = np.random.multinomial(observed_male_prop, [0.5, 0.5], line_count)[0]\n",
    "#     results = np.append(results, result)\n",
    "    \n",
    "# results\n",
    "\n",
    "# gender_stats = results\n",
    "    \n",
    "# # Visualize with a histogram\n",
    "# bpd.DataFrame().assign(gender_stats=gender_stats).plot(kind='hist', density=True, ec='w', figsize=(10, 5));\n",
    "# plt.axvline(x=observed_male_prop, color='black', linewidth=4, label='observed statistic')\n",
    "# plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52237ec0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480e4acf",
   "metadata": {},
   "source": [
    "**Question 3.5.** Compute the p-value for this hypothesis test, and save the result to `gender_p_value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8119f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_p_value = np.count_nonzero((gender_stats >= observed_male_prop)) / len(gender_stats)\n",
    "gender_p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d7e250",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3_5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215afc47",
   "metadata": {},
   "source": [
    "You should find that the p-value is above the standard cutoff of 0.05 for statistical significance. So in this case, we fail to reject the null. You'll use this information when creating your reboot - you'll assign an equal number of lines to the male main characters and the female main characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a34247e",
   "metadata": {},
   "source": [
    "**Question 3.6.** Conceptually, how would you expect the statistics in `gender_stats` to change if `line_count` were a much larger value and `observed_male_prop` were the same? What effect would that have on the result of the hypothesis test?\n",
    "\n",
    "From the options below, save the number of your choice in the variable`gender_stats_change`.\n",
    "\n",
    "1. The values in `gender_stats` would be **less spread out**. We'd be **less** likely to reject the null hypothesis if `observed_male_prop` remained the same.\n",
    "2. The values in `gender_stats` would be **less spread out**. We'd be **more** likely to reject the null hypothesis if `observed_male_prop` remained the same.\n",
    "3. The values in `gender_stats` would be **about the same**. We'd be **equally** likely to reject the null hypothesis if `observed_male_prop` remained the same.\n",
    "4. The values in `gender_stats` would be **more spread out**. We'd be **less** likely to reject the null hypothesis if `observed_male_prop` remained the same.\n",
    "5. The values in `gender_stats` would be **more spread out**. We'd be **more** likely to reject the null hypothesis if `observed_male_prop` remained the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5fac3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_stats_change = 2\n",
    "gender_stats_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5133d790",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3_6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ba2fa1",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "## Section 4: The Emotional One üòÅüò¢üò®\n",
    "\n",
    "<small>(<a href=\"#outline\">return to the outline</a>)</small>\n",
    "\n",
    "Now it's time to investigate the character dynamics of the show! In order to do this, we'll look at the `emotions` dataset. In data science, we often refer to emotion as \"sentiment.\" We only have sentiment data for certain lines from the first four seasons of the show, so we'll base our investigation on those lines only. Since we also have knowledge of probability, we'll utilize that as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa322d0a",
   "metadata": {},
   "source": [
    "Let's take a look at the data in `emotions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70295c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1db5841",
   "metadata": {},
   "source": [
    "The `emotions` DataFrame includes the emotion for lines spoken by minor characters like `'Air Hostess'`, but we want to focus on the emotions of the six main characters, whose names are included in the `main` DataFrame defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8a1d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "main = bpd.DataFrame().assign(speaker = ['Monica Geller', 'Ross Geller', 'Rachel Green', 'Chandler Bing', 'Phoebe Buffay', 'Joey Tribbiani'])\n",
    "main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b0a8fd",
   "metadata": {},
   "source": [
    "**Question 4.1.** Create a DataFrame called `main_emotions` by merging the `main` and `emotions` DataFrame so that the resulting DataFrame contains the same columns as `emotions`, but only has rows corresponding to lines spoken by one of the six main characters. The rows and columns of `main_emotions` can be in any order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c891aa3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_emotions = emotions.merge(main, on='speaker')\n",
    "main_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c20a02c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626211a0",
   "metadata": {},
   "source": [
    "We'll use `main_emotions` instead of `emotions` for the rest of this section. Let's look into how sentiment varies across the six main characters by investigating some probabilities.\n",
    "\n",
    "**Question 4.2.** If we randomly select a line spoken by `'Phoebe Buffay'` from `main_emotions`, what is the probability that the line is characterized as `'Joyful'`? Assign your answer to the variable `p_joyful_given_phoebe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60085625",
   "metadata": {},
   "outputs": [],
   "source": [
    "phoebe_df = main_emotions[main_emotions.get('speaker')=='Phoebe Buffay']\n",
    "phoebe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafe2dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Probability of randomly selecting a joyful line by Phoebe\n",
    "\n",
    "p_joyful_given_phoebe = phoebe_df[phoebe_df.get('emotion')=='Joyful'].shape[0]/phoebe_df.shape[0]\n",
    "p_joyful_given_phoebe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc6588b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Probability of randomly selecting Phoebe and a joyful line\n",
    "\n",
    "test_p_joyful_given_phoebe = main_emotions[(main_emotions.get('emotion')=='Joyful') & (main_emotions.get('speaker')=='Phoebe Buffay')].shape[0]/main_emotions.shape[0]\n",
    "test_p_joyful_given_phoebe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b2a38f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55087576",
   "metadata": {},
   "source": [
    "**Question 4.3.** It's hard to know from this probability alone whether Phoebe is a particularly joyful character compared to other characters. Let's instead answer a related question: if we randomly select a `'Joyful'` line from `main_emotions`, what is the probability that it was spoken by `'Phoebe Buffay'`? Assign your answer to the variable `p_phoebe_given_joyful`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43590226",
   "metadata": {},
   "outputs": [],
   "source": [
    "joyful_df = main_emotions[main_emotions.get('emotion')=='Joyful']\n",
    "joyful_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb5de0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p_phoebe_given_joyful = joyful_df[joyful_df.get('speaker')=='Phoebe Buffay'].shape[0]/joyful_df.shape[0]\n",
    "p_phoebe_given_joyful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f0802c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18aed975",
   "metadata": {},
   "source": [
    "Notice that in both of the previous questions, you calculated a conditional probability. First you calculated the probability of an emotion given a speaker, then you calculated the probability of a speaker given an emotion. Next, we want to do these same kinds of calculations for _all_ of the main characters and _all_ of the emotions. Let's generalize the code for these calculations so that we can more easily compute conditional probabilities with other conditions.\n",
    "\n",
    "**Question 4.4.** ‚≠ê‚≠ê Your job is to implement the function `conditional_probability`. It has two arguments, `find` and `given`, both of which are lists. Let's walk through how this works using an example. Suppose we want to compute the probability that a `'Joyful'` line is spoken by `'Phoebe Buffay'` as we did in the previous question.\n",
    "\n",
    "- `find` is a list of two elements:\n",
    "    - The first element in `find` is the column in `main_emotions` that contains the event that we are trying to find the probability of. In our example, this is `'speaker'`.\n",
    "    - The second element in `find` is the value in the aforementioned column that we're trying to find. In our example, this is `'Phoebe Buffay'`.\n",
    "- `given` is a list of two elements:\n",
    "    - The first element in `given` is the column in `main_emotions` that contains the event that is known to be true. In our example, this is `'emotion'`. \n",
    "    - The second element in `given` is the value in the aforementioned column. In our example, this is `'Joyful'`.\n",
    "\n",
    "Putting this all together, this means that `conditional_probability(['speaker', 'Phoebe Buffay'], ['emotion', 'Joyful'])` should evaluate to your answer from the previous part (but the `conditional_probability` function should work for any example, not just this one).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb4991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_probability(find, given):\n",
    "    '''Returns the conditional probability of an event given a known condition.'''\n",
    "    get_column = given[0]\n",
    "    get_value = given[1]\n",
    "    given_dataframe = main_emotions[main_emotions.get(get_column) == get_value]\n",
    "    \n",
    "    find_column = find[0]\n",
    "    find_value = find[1]\n",
    "    find_dataframe = given_dataframe[given_dataframe.get(find_column) == find_value]\n",
    "    return find_dataframe.shape[0]/given_dataframe.shape[0]\n",
    "    \n",
    "# This should evalaute to your answer to Question 4.3. Feel free to try calculating other conditional probabilities.\n",
    "conditional_probability(['speaker', 'Phoebe Buffay'], ['emotion', 'Joyful'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b53929",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditional_probability(['emotion', 'Joyful'], ['speaker', 'Phoebe Buffay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b01f690",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditional_probability(['speaker', 'Ross Geller'], ['season', 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c314ac6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec13ee0",
   "metadata": {},
   "source": [
    "**Question 4.5.** Now, use the `conditional_probability` function you just wrote to find some probabilities:\n",
    "- `p_mad_given_ross`: The probability that a line spoken by `'Ross Geller'` is `'Mad'`. \n",
    "- `p_monica_given_neutral`: The probability that a `'Neutral'` line is said by `'Monica Geller'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa17549",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_mad_given_ross = conditional_probability(['emotion', 'Mad'], ['speaker', 'Ross Geller'])\n",
    "p_monica_given_neutral = conditional_probability(['speaker', 'Monica Geller'], ['emotion', 'Neutral'])\n",
    "\n",
    "print('P(Mad given Ross) = ' + str(p_mad_given_ross))\n",
    "print('P(Monica given Neutral) = ' + str(p_monica_given_neutral))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b39341",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4_5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe641e68",
   "metadata": {},
   "source": [
    "**Question 4.6.** ‚≠ê‚≠ê Now you want to focus on a particular emotion and order the characters from most likely to say a line of that emotion to least likely. Create a function called `rank_speakers_by_emotion`that takes as input a string `emotion` representing one of the seven emotions, and a boolean `draw_plot`.\n",
    "\n",
    "If `draw_plot` is `True`, the function should plot a horizontal bar chart where the bar lengths represent the probability that a line of the given emotion is spoken by each of the the six main characters. The total length of the six bars should sum to 1 and the bars should be ordered from longest to shortest. Give your plot an appropriate title.\n",
    "\n",
    "When `draw_plot` is `False`, the function should not draw a plot.\n",
    "\n",
    "Regardless of the value of `draw_plot`, the function should return an array of the six main characters, ranked in descending order of their probabilities of saying a line of the given emotion. \n",
    "\n",
    "For example, `rank_speakers_by_emotion('Sad', True)` should behave as follows.\n",
    "<img src=\"images/sad_line.jpg\" width = 700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898f5fb5",
   "metadata": {},
   "source": [
    "***Hint:*** You can solve this problem in at least two different ways. One way involves calling your `conditional_probability` function repeatedly with different inputs. Another way doesn't use the `conditional_probability` function at all, but instead uses `groupby`. You can use either approach, or try both for extra practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d591bac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792a236d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_speakers_by_emotion(emotion, draw_plot):\n",
    "    '''Returns an ordered array of main characters, ranked by their conditional probabilities of \n",
    "    speaking a line of the given emotion. Draws a bar graph when draw_plot is True. '''\n",
    "    \n",
    "    speaker_array = np.array([])\n",
    "    prob_array = np.array([])\n",
    "    \n",
    "    emotion_df = main_emotions[main_emotions.get('emotion') == emotion]\n",
    "    grouped_emotions = emotion_df.groupby(['speaker', 'emotion']).count().sort_values(by='season', ascending=False).reset_index()\n",
    "    for speaker in grouped_emotions.get('speaker').unique():\n",
    "        speaker_df = grouped_emotions[grouped_emotions.get('speaker') == speaker]\n",
    "        speaker_array = np.append(speaker_array, speaker)\n",
    "        prob_array = np.append(prob_array, speaker_df.get('text')/emotion_df.shape[0])\n",
    "        #speaker_df = speaker_df.assign(cond_prob = speaker_df.shape[0]/emotion_df.shape[0])\n",
    "        #emotion_df = emotion_df.assign(cond_prob = emotion_df[emotion_df.get('speaker')==speaker].shape[0]/emotion_df.shape[0])\n",
    "    #emotion_df = emotion_df.groupby(['emotion', 'speaker']).count().reset_index().get(['emotion', 'speaker'])\n",
    "    \n",
    "\n",
    "    #emotion_df.assign(cond_prob = emotion_df.get(['speaker', 'emotion']).apply(conditional_probability))\n",
    "    \n",
    "\n",
    "        \n",
    "    #return prob_array #speaker_df.shape[0]/emotion_df.shape[0]#speaker_df#, emotion_df.shape[0]#speaker, prob\n",
    "    \n",
    "    speaker_prob_df = bpd.DataFrame()\n",
    "    speaker_prob_df = speaker_prob_df.assign(speaker = speaker_array)\n",
    "    speaker_prob_df = speaker_prob_df.assign(Probability = prob_array)\n",
    "    speaker_prob_df = speaker_prob_df.set_index('speaker')\n",
    "    speaker_prob_df = speaker_prob_df.sort_values(by='Probability', ascending=True)\n",
    "    \n",
    "    if draw_plot==True:\n",
    "        speaker_prob_df.plot(kind='barh', title=\"Friends Characters Ranked by Probability of a \" + emotion + \" Line\");\n",
    "    \n",
    "    #return speaker_prob_df\n",
    "    return speaker_array\n",
    "    \n",
    "rank_speakers_by_emotion('Sad', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c43ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rank_speakers_by_emotion2(emotion, draw_plot):\n",
    "#     '''Returns an ordered array of main characters, ranked by their conditional probabilities of \n",
    "#     speaking a line of the given emotion. Draws a bar graph when draw_plot is True. '''\n",
    "    \n",
    "#     prob_values = np.array([])\n",
    "#     speaker_array = np.array([])\n",
    "#     speaker_prob_list = []\n",
    "#     #speaker_and_prob_df = bpd.DataFrame()\n",
    "    \n",
    "#     for speaker in main_emotions.get('speaker').unique():\n",
    "#         prob_values = np.append(prob_values, conditional_probability(['speaker', speaker], ['emotion', emotion]))\n",
    "#         speaker_array = np.append(speaker_array, speaker)\n",
    "#         prob_values_list = prob_values.tolist()\n",
    "#         speaker_array_list = speaker_array.tolist()\n",
    "        \n",
    "#     for i in np.arange(6):\n",
    "#         speaker_prob_list = np.append(speaker_prob_list, prob_values_list[i])\n",
    "#         speaker_prob_list = np.append(speaker_prob_list, speaker_array_list[i])\n",
    "#         speaker_prob_list = speaker_prob_list.tolist()\n",
    "    \n",
    "#     appended_list = [] \n",
    " \n",
    "#     for i in range(0, len(speaker_prob_list), 2): \n",
    "#         if i + 1 < len(speaker_prob_list): \n",
    "#             pair = [speaker_prob_list[i], speaker_prob_list[i + 1]] \n",
    "#             appended_list.append(pair)\n",
    "    \n",
    "#     sorted_list = sorted(appended_list, key=lambda x: x[0])\n",
    "#     #speaker_prob_list = np.append(speaker_prob_list, prob_values_list+speaker_array_list)\n",
    "# #         speaker_prob_list = np.append(speaker_prob_list, speaker_array_list[speaker])\n",
    "        \n",
    "#         #speaker_and_prob_df = speaker_and_prob_df.assign(prob = prob_values)\n",
    "#         #speaker_and_prob_df = speaker_and_prob_df.assign(spoken_by = speaker)\n",
    "    \n",
    "#     return sorted_list#, np.array(sorted_list) #speaker_prob_list, appended_list, sorted(appended_list, key=lambda x: x[0])#prob_values_list+speaker_array_list #prob_values_list+speaker_array_list #speaker_prob_list,\n",
    "    \n",
    "# #     prob_array = np.array([])\n",
    "# #     speaker_array = np.array([])\n",
    "# #     get_speakers = main_emotions.get('speaker').unique()\n",
    "# #     for speaker in get_speakers:\n",
    "# #         get_probs = conditional_probability(['speaker', speaker], ['emotion', emotion])\n",
    "# #         prob_array = np.append(prob_array, get_probs)\n",
    "# #         speaker_array = np.append(speaker_array, speaker)\n",
    "# #     return prob_array, speaker_array\n",
    "\n",
    "# # Try to replicate the graph shown in the example.\n",
    "# rank_speakers_by_emotion2('Sad', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87576e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditional_probability(['speaker', 'Rachel Green'], ['emotion', 'Sad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7526a24",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4_6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361db6ee",
   "metadata": {},
   "source": [
    "Now, let's try using our function on all seven emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9e4593",
   "metadata": {},
   "outputs": [],
   "source": [
    "seven_emotions = main_emotions.get('emotion').unique()\n",
    "for emotion in seven_emotions:\n",
    "    print('Characters in descending order of '+emotion+': \\n', rank_speakers_by_emotion(emotion, True), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b46c80f",
   "metadata": {},
   "source": [
    "Now, you want to create a visualization that will represent the distribution of different emotions for each character. You plan to give this visualization to the writers you will hire for the reboot, as they will need to understand the emotional range of each character to write a reboot that is true to the spirit of the original *Friends* sitcom.\n",
    "\n",
    "**Question 4.7.** Your task is to create a function `emotions_by_speaker` that will take in a `speaker` and calculate the probability of that character saying a line of each of the seven emotions. The function should return a DataFrame, indexed by `'emotion'`, with one column `'probability'`, containing values that sum to 1. The rows should appear **alphabetically** by `'emotion'`\n",
    "\n",
    "For example, `emotions_by_speaker('Chandler Bing')` should return this DataFrame:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc71888b",
   "metadata": {},
   "source": [
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>probability</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>emotion</th>\n",
    "      <th></th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>Joyful</th>\n",
    "      <td>0.220890</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Mad</th>\n",
    "      <td>0.124429</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Neutral</th>\n",
    "      <td>0.306507</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Peaceful</th>\n",
    "      <td>0.096461</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Powerful</th>\n",
    "      <td>0.080479</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Sad</th>\n",
    "      <td>0.044521</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Scared</th>\n",
    "      <td>0.126712</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e44f597",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_array = np.array([])\n",
    "prob_array = np.array([])\n",
    "    \n",
    "speaker = 'Chandler Bing'\n",
    "    \n",
    "speaker_df = main_emotions[main_emotions.get('speaker') == speaker]\n",
    "grouped_speaker = speaker_df.groupby(['speaker', 'emotion']).count().sort_values(by='season', ascending=False).reset_index()\n",
    "for emotion in grouped_speaker.get('emotion').unique():\n",
    "    emotion_df = grouped_speaker[grouped_speaker.get('emotion') == emotion]\n",
    "    emotion_array = np.append(emotion_array, emotion)\n",
    "    prob_array = np.append(prob_array, emotion_df.get('text')/speaker_df.shape[0])\n",
    "        #speaker_df = speaker_df.assign(cond_prob = speaker_df.shape[0]/emotion_df.shape[0])\n",
    "        #emotion_df = emotion_df.assign(cond_prob = emotion_df[emotion_df.get('speaker')==speaker].shape[0]/emotion_df.shape[0])\n",
    "    #emotion_df = emotion_df.groupby(['emotion', 'speaker']).count().reset_index().get(['emotion', 'speaker'])\n",
    "    \n",
    "\n",
    "    #emotion_df.assign(cond_prob = emotion_df.get(['speaker', 'emotion']).apply(conditional_probability))\n",
    "    \n",
    "\n",
    "        \n",
    "    #return prob_array #speaker_df.shape[0]/emotion_df.shape[0]#speaker_df#, emotion_df.shape[0]#speaker, prob\n",
    "    \n",
    "emotion_prob_df = bpd.DataFrame()\n",
    "emotion_prob_df = emotion_prob_df.assign(emotion = emotion_array)\n",
    "emotion_prob_df = emotion_prob_df.assign(probability = prob_array)\n",
    "#emotion_prob_df = emotion_prob_df.set_index('emotion')\n",
    "emotion_prob_df = emotion_prob_df.sort_values(by='emotion', ascending=True)\n",
    "emotion_prob_df = emotion_prob_df.set_index('emotion')\n",
    "emotion_prob_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f41c28b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def emotions_by_speaker(speaker):\n",
    "    '''Returns an DataFrame of emotions and their associated probabilities for the given speaker.'''\n",
    "    emotion_array = np.array([])\n",
    "    prob_array = np.array([])\n",
    "\n",
    "    speaker_df = main_emotions[main_emotions.get('speaker') == speaker]\n",
    "    grouped_speaker = speaker_df.groupby(['speaker', 'emotion']).count().sort_values(by='season', ascending=False).reset_index()\n",
    "    for emotion in grouped_speaker.get('emotion').unique():\n",
    "        emotion_df = grouped_speaker[grouped_speaker.get('emotion') == emotion]\n",
    "        emotion_array = np.append(emotion_array, emotion)\n",
    "        prob_array = np.append(prob_array, emotion_df.get('text')/speaker_df.shape[0])\n",
    "            #speaker_df = speaker_df.assign(cond_prob = speaker_df.shape[0]/emotion_df.shape[0])\n",
    "            #emotion_df = emotion_df.assign(cond_prob = emotion_df[emotion_df.get('speaker')==speaker].shape[0]/emotion_df.shape[0])\n",
    "        #emotion_df = emotion_df.groupby(['emotion', 'speaker']).count().reset_index().get(['emotion', 'speaker'])\n",
    "\n",
    "\n",
    "        #emotion_df.assign(cond_prob = emotion_df.get(['speaker', 'emotion']).apply(conditional_probability))\n",
    "\n",
    "\n",
    "\n",
    "        #return prob_array #speaker_df.shape[0]/emotion_df.shape[0]#speaker_df#, emotion_df.shape[0]#speaker, prob\n",
    "\n",
    "    emotion_prob_df = bpd.DataFrame()\n",
    "    emotion_prob_df = emotion_prob_df.assign(emotion = emotion_array)\n",
    "    emotion_prob_df = emotion_prob_df.assign(probability = prob_array)\n",
    "    #emotion_prob_df = emotion_prob_df.set_index('emotion')\n",
    "    emotion_prob_df = emotion_prob_df.sort_values(by='emotion', ascending=True)\n",
    "    emotion_prob_df = emotion_prob_df.set_index('emotion')\n",
    "    return emotion_prob_df\n",
    "\n",
    "# An example call to your function. Feel free to change this and try out other speakers.\n",
    "emotions_by_speaker('Chandler Bing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa23ea4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4_7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dae0e9",
   "metadata": {},
   "source": [
    "**Question 4.8.** Now, we'll use your function to create a visualization of speaker emotions, which should show the similarities and differences between the types of emotions each character uses when they speak. We've completed the visualization code for you, you just need to fill in a few missing lines as indicated by the comments. You do not need to know how the code for this visualization works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e560f3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "main.get('speaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0126bff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "main.get('speaker').unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f16ea6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_emotions.get('speaker').unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baf00e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_name_test(character):\n",
    "    # Return the first name of the input character's name, converted to lowercase.\n",
    "    # For example, RacheL Green -> rachel.\n",
    "    lower_case_character = character.lower()\n",
    "    return lower_case_character.split(\" \")[0]\n",
    "\n",
    "first_name_test('Ross Geller')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f25cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set characters to an array of the names of the six main characters.\n",
    "characters = main_emotions.get('speaker').unique()\n",
    "\n",
    "dfs = {}\n",
    "for speaker in characters:\n",
    "    # Call your emotions_by_speaker function on each speaker, and save the result as dfs[speaker].\n",
    "    dfs[speaker] = emotions_by_speaker(speaker)\n",
    "\n",
    "def first_name(character):\n",
    "    # Return the first name of the input character's name, converted to lowercase.\n",
    "    # For example, RacheL Green -> rachel.\n",
    "    lower_case_character = character.lower()\n",
    "    return lower_case_character.split(\" \")[0]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "max_height = max_width = 0\n",
    "for character in characters:\n",
    "    img = plt.imread(f'images/face_pics/{first_name(character)}.png')\n",
    "    h, w, _ = img.shape\n",
    "    max_height = max(max_height, h)\n",
    "    max_width = max(max_width, w)\n",
    "\n",
    "for ax, character in zip(axes, characters):\n",
    "    df = dfs[character]\n",
    "    pie, _, _ = ax.pie(df.get('probability'), labels=df.index, startangle=90, autopct='%1.1f%%', pctdistance=0.85, normalize=False)\n",
    "    ax.set_title(character)\n",
    "\n",
    "    img = plt.imread(f'images/face_pics/{first_name(character)}.png')\n",
    "\n",
    "    h, w, _ = img.shape\n",
    "    radius = min(h, w) // 2\n",
    "    y, x = np.ogrid[:h, :w]\n",
    "    mask = ((y - h // 2) ** 2 + (x - w // 2) ** 2) > radius ** 2\n",
    "    img[mask] = [1, 1, 1, 0]  \n",
    "\n",
    "    zoom_factor = max_height / h if h > w else max_width / w\n",
    "    imagebox = OffsetImage(img, zoom=zoom_factor * 0.43)\n",
    "    ab = AnnotationBbox(imagebox, (0.5, 0.5), frameon=False, pad=0, xycoords='axes fraction', boxcoords='axes fraction')\n",
    "    ax.add_artist(ab)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba129f03",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4_8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483fc4ba",
   "metadata": {},
   "source": [
    "This visualization will serve as a great template for the writers of the new reboot!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143e1ad1",
   "metadata": {},
   "source": [
    "<a id='section5'></a>\n",
    "## Section 5: The One with the Spoilers üôä\n",
    "\n",
    "<small>(<a href=\"#outline\">return to the outline</a>)</small>\n",
    "\n",
    "In this section, we will use permutation testing to compare different groups of episodes from *Friends* in terms of their viewership and ratings. This might help us make some decisions about what kinds of episodes our reboot should include to boost viewership and ratings! \n",
    "\n",
    "We'll start by looking at how earlier seasons of *Friends* (seasons 1-5) compared to later seasons (seasons 6-10) in terms of viewership. Check out the information you'll be working with in the `episodes` DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df81638",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0edee4",
   "metadata": {},
   "source": [
    "You'll notice that we have data about viewership and rating for 236 episodes (10 seasons). Let's start by labeling the episodes in `episodes` to make it easier to answer our question: \n",
    "> *Which half of the show has more viewers?*\n",
    "\n",
    "**Question 5.1.** Create a DataFrame called `halves` which has the same data as `episodes` and an additional column called `'which_half'`. In this column, values should be either `'first half'` (for seasons 1-5) or `'second half'` (for seasons 6-10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf71a59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def which_half_func(value):\n",
    "    if value <= 5:\n",
    "        return 'first half'\n",
    "    else: \n",
    "        return 'second half'\n",
    "\n",
    "halves = episodes.assign(which_half = episodes.get('season').apply(which_half_func))\n",
    "halves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48236e5f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc6123d",
   "metadata": {},
   "source": [
    "**Question 5.2.** Before we start permutation testing, let's visualize the difference in viewership between the first half and second half of *Friends* episodes. Create an overlaid density histogram that compares the distribution of `'us_views_millions'` for first half episodes with that of second half episodes. Add a title, legend, and appropriate axis labels to your plot.\n",
    "\n",
    "Then, assign the difference in **average** viewership (in millions) between the two groups to the variable <code>view_difference</code>. Here, `view_difference` should represent how much **greater** the first half viewership is than the second half, on average.\n",
    "\n",
    "***Hint:***  Refer to the smoking and birth weight example in [Lecture 22](https://dsc10.com/resources/lectures/lec22/lec22.html#Visualizing-the-distribution-of-each-group) for help creating the overlaid histogram! Think about what bin size is best suited to show the differences between these two distributions. You might want to tweak the bins until the differences become clearer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce842b30",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5_2\n",
    "points: 1\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79483aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_views_per_half = halves.get(['which_half', 'us_views_millions'])\n",
    "\n",
    "first_half_df = get_views_per_half[get_views_per_half.get('which_half')=='first half']\n",
    "second_half_df = get_views_per_half[get_views_per_half.get('which_half')=='second half']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8ccd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_half_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a756bfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_half_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1aea413",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "view_bins = np.arange(15, 55, 2)\n",
    "first_half_df.plot(kind='hist', density=True, ax=ax, alpha=0.75, bins=view_bins, ec='w', figsize=(10, 5))\n",
    "second_half_df.plot(kind='hist', density=True, ax=ax, alpha=0.75, bins=view_bins, ec='w')\n",
    "plt.legend(['First Half', 'Second Half'])\n",
    "plt.xlabel('Views in Millions');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adeb1b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "halves.groupby('which_half').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df0559d",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_half_df.get('us_views_millions').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833b5a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_half_df.get('us_views_millions').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1905c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_difference = first_half_df.get('us_views_millions').mean() - second_half_df.get('us_views_millions').mean()\n",
    "view_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613b88fa",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd904ca8",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "You should see that viewership was higher in the first half of *Friends* than in the second. Is this difference significant or was it just by chance? Let's perform a permutation test to find out.\n",
    "\n",
    "**Question 5.3.** ‚≠ê‚≠ê Let's clean up our DataFrame to focus only on the columns `'us_views_millions'` and `'which_half'`. To start, create a DataFrame <code>'views_by_group'</code> that only includes these two columns from `halves`. \n",
    "\n",
    "We would like to know whether the first half of episodes had significantly **more** viewership than the second half on average. Think about what your null and alternative hypotheses would be to answer this question. \n",
    "\n",
    "In the previous question, you computed the *observed statistic*, the average number of views by which the first half exceeds the second half. Perform a permutation test, simulating 1000 differences and storing them in the array <code>simulated_differences</code>. \n",
    "\n",
    "Once you're done, generate a density histogram of the simulated differences, which also shows your observed statistic for comparison. See [Lecture 19](https://dsc10.com/resources/lectures/lec19/lec19.html#Step-3-%E2%80%93-Visualize-the-resulting-distribution) for an example of the type of plot you'll need to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d491de3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "views_by_group = halves.get(['which_half', 'us_views_millions'])\n",
    "\n",
    "observed_stat = view_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddf252f",
   "metadata": {},
   "outputs": [],
   "source": [
    "permutated_views = views_by_group.assign(permutated_half = np.random.permutation(views_by_group.get('which_half')))\n",
    "permutated_views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074fbf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_differences = np.array([])\n",
    "for i in range(1000):\n",
    "    shuffled_test_df = views_by_group.assign(permutated_half = np.random.permutation(views_by_group.get('which_half')))\n",
    "    \n",
    "    shuffled_first_half = shuffled_test_df[shuffled_test_df.get('permutated_half') == 'first half']\n",
    "    shuffled_first_half_mean = shuffled_first_half.get('us_views_millions').mean()\n",
    "    \n",
    "    shuffled_second_half = shuffled_test_df[shuffled_test_df.get('permutated_half') == 'second half']\n",
    "    shuffled_second_half_mean = shuffled_second_half.get('us_views_millions').mean()\n",
    "\n",
    "    tested_difference = shuffled_first_half_mean - shuffled_second_half_mean\n",
    "    simulated_differences = np.append(simulated_differences, tested_difference)\n",
    "\n",
    "simulated_differences    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633614c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_differences.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0201679f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpd.DataFrame().assign(simulated_differences=simulated_differences).plot(kind='hist', bins= np.arange(-2.2, 2.2, 0.5),density=True, ec='w', figsize=(10, 5));\n",
    "plt.axvline(x=observed_stat, color='black', linewidth=4, label='observed difference in means')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab56f37a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6cca3e",
   "metadata": {},
   "source": [
    "**Question 5.4.** Calculate a p-value for this permutation test using the observed statistic and the simulated differences, and assign the result to `p_halves`. Then, interpret these results at the 0.05 significance level by assigning <code>True</code> to the variable <code>significant_difference</code> if you believe we can **reject** the null hypothesis and <code>False</code> otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97565787",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_p_value_means = bpd.DataFrame()\n",
    "get_p_value_means = get_p_value_means.assign(simulated_diff = simulated_differences)\n",
    "greater_than_observed_means = get_p_value_means.get('simulated_diff') >= observed_stat\n",
    "get_p_value_means = get_p_value_means.assign(greater_than_observed_means = greater_than_observed_means)\n",
    "#greater_than_observed\n",
    "get_p_value_means#.get('smaller_than_observed_means').unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defb7988",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_halves = get_p_value_means[get_p_value_means.get('simulated_diff')>=observed_stat].shape[0] / get_p_value_means.shape[0]\n",
    "p_halves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd73d46",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5_4_a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd8bcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_difference = True\n",
    "significant_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9300b003",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5_4_b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57598f3",
   "metadata": {},
   "source": [
    "We've answered our first question with permutation testing, which helped us figure out whether there was a significant difference in viewership between two groups of episodes. We can use this insight when determining how many episodes our reboot should have. We'll explore the length of our reboot more in Section 7 of this project.\n",
    "\n",
    "Now, let's ask a different question that can be solved by the same tools: \n",
    "> *Do people prefer episodes where at least two of the main characters are in a romantic relationship over episodes where there aren't any couples?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b4bc41",
   "metadata": {},
   "source": [
    "First, we need to label which episodes have an ongoing romantic relationship. \n",
    "\n",
    "<b><font color='red'>‚ö† Spoiler alert! ‚ö†</font></b>\n",
    "\n",
    "In the show, there are two couples that each date for a period of time. \n",
    " - Ross and Rachel are in a relationship from Season 2, Episode 14 to Season 3, Episode 15, inclusive.\n",
    " - Monica and Chandler are in a relationship from Season 4, Episode 24 to Season 10, Episode 18, inclusive, which is through the end of the show.\n",
    " \n",
    "**Question 5.5.** ‚≠ê‚≠ê Create a new DataFrame called `relationship` that has the same information as `episodes` and an additional column called `'couple'` that contains `True` if the episode falls in one of the intervals mentioned above, and `False` otherwise. \n",
    "\n",
    "***Hints:*** \n",
    "- To solve this problem with `apply`, you need to get around the limitation that you can only `apply` a function of one parameter. Consider writing a function that takes strings of the form `'x,y'` where `x` represents the season number and `y` represents the episode number.\n",
    "- It is also possible to solve this problem without using `apply`. For a challenge, try both ways and see which way is simpler!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8b0fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caebc02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_episode = episodes.assign(season_episode = episodes.get('season').apply(str) + ', ' + episodes.get('episode').apply(str))#+str(episodes.get('episode')))\n",
    "season_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e69c569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_couple(season_ep_string):\n",
    "    split_string = season_ep_string.split(', ')\n",
    "    season = int(split_string[0])\n",
    "    episode = int(split_string[1])\n",
    "    if (season == 2 and episode >= 14) or (season == 3 and episode <= 14) or (season == 4 and episode >= 24) or (season >= 5):\n",
    "        return True \n",
    "    else:\n",
    "        return False #int(split_string[0])\n",
    "\n",
    "has_couple('1, 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603c70d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "relationship = season_episode.assign(couple = season_episode.get('season_episode').apply(has_couple))\n",
    "relationship = relationship.drop(columns=['season_episode'])\n",
    "relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3623b185",
   "metadata": {},
   "outputs": [],
   "source": [
    "relationship[(relationship.get('season')==4)]#&(relationship.get('season')==4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a1b8c8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5_5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19129f92",
   "metadata": {},
   "source": [
    "**Question 5.6.**  Now, let's state the hypotheses for our permutation test to answer the question:\n",
    "> *Do episodes where characters **are** in a relationship have higher ratings than those where characters **aren't** in a relationship?*\n",
    "\n",
    "Consider the following choices for statements of the null and alternative hypotheses. For simplicity, let us call the relationship group \"R\" (for \"relationship\") and the non-relationship group \"S\" (for \"single\"). \n",
    "\n",
    "1. There is no difference in average episode rating between groups R and S.</li>\n",
    "1. There is a difference in average episode rating between groups R and S.</li>\n",
    "1. Group R has a higher rating than Group S, on average.</li>\n",
    "1. Group S has a higher rating than Group R, on average.</li>\n",
    "\n",
    "Set `null_choice` to a number 1 through 4, corresponding to your choice for the null hypothesis. Set `alternative_choice` to a number 1 through 4, corresponding to your choice for the alternative hypothesis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec9e884",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_choice = 1           \n",
    "alternative_choice = 3    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce511cb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5_6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254a6584",
   "metadata": {},
   "source": [
    "**Question 5.7.** Using the <code>relationship</code> DataFrame, compute the *observed statistic* representing the difference in mean rating between groups of episodes that have relationships (R) versus those that don't (S), in the order of R minus S. Store your result in the variable <code>couple_observed</code>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06f5d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_relationship_mean = relationship[relationship.get('couple')==True].get('imdb_rating').mean()\n",
    "R = has_relationship_mean\n",
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346031ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "hasnt_relationship_mean = relationship[relationship.get('couple')==False].get('imdb_rating').mean()\n",
    "S = hasnt_relationship_mean\n",
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a97dd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "couple_observed = R-S\n",
    "couple_observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70665a9a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5_7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ae344f",
   "metadata": {},
   "source": [
    "**Question 5.8.** Now it's time to run the permutation test. Similarly to how you did in Question 5.3, simulate 1000 differences of group means, storing them in the array `couple_stats`. Then, plot your simulated differences in rating and show how they compare to your observed statistic. Make sure you calculate your simulated differences by subtracting in the same order as you did when calculating your observed statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461c0524",
   "metadata": {},
   "outputs": [],
   "source": [
    "couple_stats = np.array([])\n",
    "for i in range(1000):\n",
    "    shuffled_coupled_df = relationship.assign(permutated_couple_half = np.random.permutation(relationship.get('couple')))\n",
    "    \n",
    "    shuffled_has_couple = shuffled_coupled_df[shuffled_coupled_df.get('permutated_couple_half') == True]\n",
    "    shuffled_has_couple_mean = shuffled_has_couple.get('imdb_rating').mean()\n",
    "    \n",
    "    shuffled_has_no_couple = shuffled_coupled_df[shuffled_coupled_df.get('permutated_couple_half') == False]\n",
    "    shuffled_has_no_couple_mean = shuffled_has_no_couple.get('imdb_rating').mean()\n",
    "\n",
    "    rating_difference = shuffled_has_couple_mean - shuffled_has_no_couple_mean\n",
    "    couple_stats = np.append(couple_stats, rating_difference)\n",
    "\n",
    "couple_stats    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65742357",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpd.DataFrame().assign(couple_stats=couple_stats).plot(kind='hist', density=True, ec='w', figsize=(10, 5));\n",
    "plt.axvline(x=couple_observed, color='black', linewidth=4, label='observed difference in rating means')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4a3f23",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5_8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcad9b5",
   "metadata": {},
   "source": [
    "**Question 5.9.** Calculate your p-value using the observed statistic and simulated differences, storing the result in `p_couple`. Assign <code>True</code> to the variable <code>couple_claim</code> if you believe we can **reject** the null hypothesis at the 0.05 significance level and <code>False</code> otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3070f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_p_rating_means = bpd.DataFrame()\n",
    "get_p_rating_means = get_p_rating_means.assign(simulated_diff = couple_stats)\n",
    "greater_than_observed_means = get_p_rating_means.get('simulated_diff') >= couple_observed\n",
    "get_p_rating_means = get_p_rating_means.assign(greater_than_observed_means = greater_than_observed_means)\n",
    "#greater_than_observed\n",
    "get_p_rating_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75a6f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_couple = get_p_rating_means[get_p_rating_means.get('simulated_diff')>=couple_observed].shape[0] / get_p_rating_means.shape[0]\n",
    "p_couple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab3ac22",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5_9_a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3536626",
   "metadata": {},
   "outputs": [],
   "source": [
    "couple_claim = True\n",
    "couple_claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e6fe55",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5_9_b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa2059d",
   "metadata": {},
   "source": [
    "Although we can't determine from our permutation test whether characters being in a relationship *causes* better ratings, we can detect an association. You'll use this information when planning the storyline of your reboot to determine whether to include a romance angle! üíò"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b976e3",
   "metadata": {},
   "source": [
    "<a id='section6'></a>\n",
    "## Section 6: The One with the Generated Episode Titles üñ®Ô∏è\n",
    "\n",
    "<small>(<a href=\"#outline\">return to the outline</a>)</small>\n",
    "\n",
    "In this section, we'll create a tool that randomly generates episode titles for our reboot, based on the words used in the titles of *Friends* episodes. To start, let's see how *Friends* episode titles are formatted. The cell below saves all the *Friends* episode titles in an array called `titles` and prints a random sample of 10 titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea41bbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = np.array(episodes.get('title'))\n",
    "for i in np.arange(10):\n",
    "    print(np.random.choice(titles, replace=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937c925d",
   "metadata": {},
   "source": [
    "As you can see, most episode titles describe a major plot point of that episode and start with a phrase such as `'The One Where'` or `'The One with'`. In other words, the title of an episode is how you might describe the episode to someone else."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccd92f0",
   "metadata": {},
   "source": [
    "When generating episode titles for our reboot, we want to use many of the same words that appear in the titles of *Friends* episodes, so that our reboot titles stay true to the spirit of the original show and follow a similar structure. The simplest way to do this is to randomly select words, one at a time and independently, from the set of words included `titles`.\n",
    "\n",
    "The variable `every_word` defined below is an array that contains all of the words in all of the *Friends* episode titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388e2ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "every_word = np.array((' ').join(titles).split(' '))\n",
    "every_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41abc49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# words_list = ['The', 'Pilot', 'The', 'The', 'Last', 'One']\n",
    "# words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e3724b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for word in words_list:\n",
    "#     np.count(words_list, return_counts=True)\n",
    "\n",
    "np.count_nonzero(every_word == 'with')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0e5bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.unique(every_word, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5d4c52",
   "metadata": {},
   "source": [
    "Some words appear in `every_word` much more frequently than others. We'd want these more common words, like `'The'` to appear more frequently in our generated titles, since they appear more frequently in actual *Friends* titles. Let's generate our titles randomly, one word at a time, so that each word is independently selected with a probability proportional to how often the word appears in a *Friends* title."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba87612e",
   "metadata": {},
   "source": [
    "**Question 6.1.**  Complete the implementation of the function `find_proportion` which takes as input a word, and returns the proportion of words in `every_word` that are equal to the input word. This should be case-sensitive!\n",
    "\n",
    "Then calculate the proportion of words that are equal to `'Rachel'` and `'One'`, and save your results in `rachel_prop` and `one_prop`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f1fe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_proportion(word):\n",
    "    '''Returns the proportion of words equal to the input word.'''\n",
    "    return np.count_nonzero(every_word == word)/len(every_word)\n",
    "\n",
    "rachel_prop = find_proportion('Rachel')\n",
    "one_prop = find_proportion('One')\n",
    "print('Rachel:', rachel_prop )\n",
    "print('One:', one_prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38ce044",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddcc962",
   "metadata": {},
   "source": [
    "We can interpret each word's proportion of usages as a probability of that word being selected when we randomly generate a reboot title. For example, if we generate a title by independently selecting three words, we can calculate the probability of generating the title `'Rachel One Rachel'` as follows, using the [multiplication rule for independent events](https://dsc10.com/resources/lectures/lec11/lec11_11am.pdf):\n",
    "\n",
    "$$\n",
    "P(\\text{Rachel One Rachel}) = P(\\text{Rachel}) * P(\\text{One}) * P(\\text{Rachel})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688cf0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_rachel_one_rachel = rachel_prop * one_prop * rachel_prop\n",
    "prob_rachel_one_rachel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903e5cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "1/50000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3247ce",
   "metadata": {},
   "source": [
    "That's about a 1 in 50,000 chance that we'll generate the title `'Rachel One Rachel'`, which is so small primarily because `'Rachel'` is a rare word. The title `'One One One'` has a much larger chance of being generated, about 1 in 200, since `'One'` is a common word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77417a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_one_one_one = one_prop ** 3\n",
    "prob_one_one_one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4568b36a",
   "metadata": {},
   "source": [
    "While the strategy we've outlined gives a way of randomly generating reboot titles so that more common words are more likely to appear in the reboot title, it is unsatisfactory in many ways. Primarily, with this strategy, we can get titles like `'Rachel One Rachel'` and `'One One One'`, which not only don't make sense, but don't follow the structure of *Friends* episode titles that started with phrases like `'The One Where'`.\n",
    "\n",
    "Instead of considering the probabilities of individual words, let's consider the probability of **pairs** of consecutive words. For example, the pair of words `'The One'` is quite common and the pair of words `'The Last'` is quite rare. This says that when we generate a reboot title randomly one word at a time, if we start with the word `'The'`, we should be more likely to follow it with `'One'` than `'Last'`. \n",
    "\n",
    "We'll work on calculating the probability distribution of words that immediately follow a given word in *Friends* episode titles. As before, this is case-sensitive. For example, if the word is `'The'`, we'd want to look for want words that follow `'The'` only, not words that follow `'the'`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02976fec",
   "metadata": {},
   "source": [
    "**Question 6.2.** ‚≠ê‚≠ê Complete the implementation of the function `next_word` which takes as input a single episode `title` and a `word`, both strings. The function should return an array containing every word in `title` that appears immediately after `word`, or an empty array if there are no such words. This could happen either because the `word` is the last word in `title`, or because the `word` is not a part of `title` at all. The output array can contain duplicates if the same word follows the input `word` multiple times. \n",
    "\n",
    "Example behavior is shown below.\n",
    "\n",
    "```py\n",
    ">>> next_word('The One Where the Monkey Gets Away', 'Monkey')\n",
    "array(['Gets'])\n",
    "\n",
    ">>> next_word('The One with the Chicken Pox', 'Pox')\n",
    "array([])\n",
    "\n",
    ">>> next_word('This is the final project of the course', 'the')\n",
    "array(['final', 'course']\n",
    "\n",
    ">>> next_word('This is the final project of the course and there is also the final exam', 'the')\n",
    "array(['final', 'course', 'final']\n",
    "\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be87f655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_word(title, word):\n",
    "    '''Given a title and an input word, return an array of all \n",
    "    words in the title that immediately follow the input word.'''\n",
    "    next_word_array = np.array([])\n",
    "    split_title = title.split(' ')\n",
    "    for i in range(len(split_title)):\n",
    "        if split_title[i] == split_title[-1]:\n",
    "            break\n",
    "        elif split_title[i] == word:\n",
    "            next_word_array = np.append(next_word_array, split_title[i+1])\n",
    "    #test_string = split_title.split(word, 1)[1]\n",
    "    return next_word_array #split_title\n",
    "\n",
    "# An example call to your function. Feel free to change this and try out other inputs.\n",
    "next_word('The One Where the Monkey Gets Away', 'Monkey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d968995",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_word('This is the final project of the course', 'the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491f8e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_word('This is the final project of the course and there is also the final exam', 'the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6804ff20",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_word('The One After the Superbowl', 'Superbowl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ec4dc3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51fbca5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Question 6.3.** Now that we know how to find the subsequent words in a single title, let's extend what we've done to find the subsequent words across all titles in `titles`. Complete the function `words_following`, which takes as input a single `word` and returns an array of all words that immediately follow `word` in a *Friends* episode title. To do this, you should call your `next_word` function once on each title.\n",
    "\n",
    "As before, the output array can contain duplicates if the same word follows `word` multiple times, and it can be empty if no word follows `word`.\n",
    "\n",
    "***Hint:*** You can use `np.append` to append a whole array of values onto an existing array, in the same way that you would append a single value onto an existing array.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q6_3\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad721e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_following(word):\n",
    "    '''Given an input word, return an array of all words that \n",
    "    immediately follow the input word in episode titles in titles.'''\n",
    "    titles_array = np.array([])\n",
    "    following_words_array = np.array([])\n",
    "    get_titles_with_word = episodes[episodes.get('title').str.contains(word)]\n",
    "    titles = get_titles_with_word.get('title')\n",
    "    titles_array = np.append(titles_array, titles)\n",
    "    for title in titles_array:\n",
    "        split_title = title.split(' ')\n",
    "        for i in range(len(split_title)):\n",
    "            if split_title[i] == split_title[-1]:\n",
    "                #following_words_array = np.append(following_words_array, '')\n",
    "                i = i+1\n",
    "            elif split_title[i] == word:\n",
    "                following_words_array = np.append(following_words_array, split_title[i+1])\n",
    "        #following_words_array = np.append(following_words_array, split_title)\n",
    "    return following_words_array\n",
    "\n",
    "# An example call to your function. Feel free to change this and try out other input words.\n",
    "words_following('One')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e09432a",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_following('Rachel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b79bed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_titles_with_rachel = episodes[episodes.get('title').str.contains('Rachel')]\n",
    "get_titles_with_rachel.iloc[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac5636d",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_titles_with_rachel = episodes[episodes.get('title').str.contains('Rachel')]\n",
    "rachels_tiles = get_titles_with_rachel.get('title')\n",
    "rachels_tiles\n",
    "for title in rachels_tiles.to_numpy():\n",
    "    test_titles = title.split(' ')\n",
    "    print(test_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d894e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_titles_with_one = episodes[episodes.get('title').str.contains('One')]\n",
    "one_tiles = get_titles_with_one.get('title')\n",
    "one_tiles\n",
    "for title in one_tiles.to_numpy():\n",
    "    test_titles = title.split(' ')\n",
    "    print(test_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b684c7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_titles_with_superbowl = episodes[episodes.get('title').str.contains('Superbowl')]\n",
    "get_titles_with_superbowl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3db268",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66af80e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now that we know how to find all the words that follow a given word, we can create a probability distribution, where the probability of each subsequent word is just the proportion of times that word follows the given word. \n",
    "\n",
    "**Question 6.4.** ‚≠ê‚≠ê Complete the `find_prob_distribution` function which takes as input a single `word` and returns a DataFrame indexed by subsequent words that can immediately follow `word`, and with one column called `'prob'` that contains the proportion of times the subsequent word follows `word`. Sort the rows in descending order of `'prob'` so that the most likely words are at the top of the DataFrame.\n",
    "\n",
    "For example, `find_prob_distribution('The')` should look like this:\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>prob</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>word</th>\n",
    "      <th></th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>One</th>\n",
    "      <td>0.987288</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Last</th>\n",
    "      <td>0.008475</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Pilot</th>\n",
    "      <td>0.004237</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q6_4\n",
    "points: 2\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213ee7f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_prob_distribution(word):\n",
    "    '''Returns a DataFrame containing the probability distribution\n",
    "    of words that can follow the given input word.'''\n",
    "    words_array = np.array([])\n",
    "    prob_of_following_array = np.array([])\n",
    "    list_of_following_words = words_following(word)\n",
    "    for following_word in list_of_following_words:\n",
    "        words_array = np.append(words_array, following_word)\n",
    "        prob_of_word = np.count_nonzero(list_of_following_words == following_word)/len(list_of_following_words)\n",
    "        prob_of_following_array = np.append(prob_of_following_array, prob_of_word)\n",
    "        \n",
    "    following_words_df = bpd.DataFrame()\n",
    "    following_words_df = following_words_df.assign(word = words_array)\n",
    "    following_words_df = following_words_df.assign(prob = prob_of_following_array)\n",
    "    #following_words_df = following_words_df.sort_values(by='prob', ascending=False)\n",
    "    \n",
    "    following_words_df = following_words_df.groupby(['prob', 'word']).count().reset_index()\n",
    "    following_words_df = following_words_df.sort_values(by='prob', ascending=False)\n",
    "    following_words_df = following_words_df.set_index('word')\n",
    "    return following_words_df #words_array, prob_of_following_array\n",
    "\n",
    "# An example call to your function. Feel free to change this to try out other input words.\n",
    "find_prob_distribution('The')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27658b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_prob_distribution('Rachel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059f70cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_prob_distribution('Superbowl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2772f3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8055e670",
   "metadata": {},
   "source": [
    "Now, we can use these probability distributions to generate titles randomly. We'll start each title with the word `'The'`, since all *Friends* episodes start with this word. Then, the next word will be `'One'` with probability 0.987288, or `'Last'` with probability 0.008475, or `'Pilot'` with probability 0.004237. Almost certainly, the next word will be `'One'`, and then we'll figure out the next word according to the probabilities in the DataFrame below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c644cb60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "find_prob_distribution('One')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c10701",
   "metadata": {},
   "source": [
    "Perhaps by random chance we'll end up with the next word being `'Proposes'` (though it's quite unlikely). To find the words that could follow `'Proposes'` along with their associated probabilities, we'll use this DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23650084",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "find_prob_distribution('Proposes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88b6df6",
   "metadata": {},
   "source": [
    "Since this DataFrame has no rows, it means `'Proposes'` never appears in a *Friends* episode title with a word after it. That is, it only appears as the last word in an episode title. This means no word should come after it, so we're done generating our title, which in this example was `'The One Proposes'`. \n",
    "\n",
    "When we generate titles based on pairs of words in this way, the probability of generating `'The One Proposes'` can be computed as:\n",
    "\n",
    "$$\n",
    "P(\\text{The One Proposes}) = P(\\text{The}) * P(\\text{One}|\\text{The}) * P(\\text{Proposes}|\\text{One})\n",
    "$$\n",
    "\n",
    "Since all episode titles start with `'The'`, $P(\\text{The}) = 1$. The other probabilities are conditional probabilities based on the previous word. For example, $P(\\text{One}|\\text{The})$, which we read as \"the probability of `'One'` given `'The'`\", represents the probability of seeing the word `'One'` immediately after the word `'The'`. According to our `find_prob_distribution` function, this probability is quite high at 0.987288."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b9cf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_prob_distribution('The') # This says P(One|The) = 0.987288."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1420b872",
   "metadata": {},
   "source": [
    "Similarly, we can find $P(\\text{Proposes}|\\text{One})$ is very small, only 0.004274."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e01a24b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "find_prob_distribution('One') # This says P(Proposes|One) = 0.004274"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643abad3",
   "metadata": {},
   "source": [
    "Therefore, the probability of generating `'The One Proposes'` is:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(\\text{The One Proposes}) &= P(\\text{The}) * P(\\text{One}|\\text{The}) * P(\\text{Proposes}|\\text{One}) \\\\\n",
    "&= 1 * 0.987288 * 0.004274 \\\\\n",
    "&\\approx 0.004219\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4312af5b",
   "metadata": {},
   "source": [
    "This is about 1 in 2000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577fefae",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Question 6.5.** Now that we know how to generate titles based on pairs of words, let's write a function that takes no inputs, and returns a randomly generated title by starting with the word `'The'`, then adding a new word based on the conditional probabilities of words that can follow `'The'`. We'll continue adding new words after each previous word, until we encounter a word that never had any words after it in an episode title (like `'Proposes'` in our example above). At that point, we'll stop generating new words. \n",
    "\n",
    "Since we don't know how many words we'll be adding in advance, it's hard to implement this with a `for`-loop, since we don't know how many iterations we'll need. Instead, we'll use what's known as a `while`-loop, which runs continuously, with as many iterations as needed, until some stopping condition is met. In our case, we'll keep adding new words as long as the DataFrame returned by `find_prob_dist` has at least one row, since that means there are more words we can add to our title. Our stopping condition, therefore, is when `find_prob_dist` produces a DataFrame with no rows. \n",
    "\n",
    "We've implemented the `while`-loop for you. Your task is to fill in the missing lines so that `generate_title` returns a randomly generated episode title. Some comments are included to help you.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q6_5\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf2ddda",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_dist_df = find_prob_distribution('The')\n",
    "prob_dist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9cdef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_word = np.random.choice(prob_dist_df.reset_index().get('word'), 1, replace=False, p=prob_dist_df.get('prob'))[0]\n",
    "#new_word = new_word[0]\n",
    "new_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab1ab11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_title():\n",
    "\n",
    "    generated_title = 'The'\n",
    "    prob_dist_df = find_prob_distribution('The')\n",
    "\n",
    "    # Keep generating new words for our title so long as there are \n",
    "    # words that can follow the most recent word we've added.\n",
    "    while (prob_dist_df.shape[0] >= 1):\n",
    "        \n",
    "        # Set new_word to a random selection of the words represented in prob_dist_df, \n",
    "        # chosen according to their probabilities in prob_dist_df.\n",
    "        ### new_word = prob_dist_df.sample().reset_index().get('word').iloc[0]\n",
    "        new_word = np.random.choice(prob_dist_df.reset_index().get('word'), 1, replace=False, p=prob_dist_df.get('prob'))[0]\n",
    "        \n",
    "        # Add onto your generated title in the variable generated_title. \n",
    "        # Make sure to include spaces between words.\n",
    "        generated_title = generated_title + ' ' + new_word\n",
    "        \n",
    "        # Update prob_dist_df so that it contains the probability \n",
    "        # distribution of the word you just added to your title.\n",
    "        prob_dist_df = find_prob_distribution(new_word)\n",
    "\n",
    "    return generated_title\n",
    "\n",
    "generate_title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d973276",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6_5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c411587",
   "metadata": {},
   "source": [
    "**Question 6.6.** Suppose you generate one title using the `generate_title()` function above. What is the probability that the title you generate is `'The One in Vegas'`? Set your answer to `vegas_prob`.\n",
    "\n",
    "***Note:*** This is a probability question that you should answer based on the conditional probability of seeing one word given the previous. You should not do any simulation for this question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8773028a",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_prob_distribution('The') # This says P(One|The) = 0.987288"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c1c673",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_prob_distribution('One') # This says P(in|One) = 0.021368"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0ebd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_prob_distribution('in') # This says P(Vegas|in) = 0.285714"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8967b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_prob_distribution('The').loc['One'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23689b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_prob_distribution('One').loc['in'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3305d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_prob_distribution('in').loc['Vegas'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df7449d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vegas_prob = 1 * find_prob_distribution('The').loc['One'].iloc[0] * find_prob_distribution('One').loc['in'].iloc[0] * find_prob_distribution('in').loc['Vegas'].iloc[0]\n",
    "vegas_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c90f71",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6_6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea7c994",
   "metadata": {},
   "source": [
    "**Question 6.7.** You want to know how frequently your `generate_title()` function generates a title that actually is a real title for a *Friends* episode. Based on a simulation with 1,000 trials, estimate the probability of a generated title being a real *Friends* title, and store the result in `prob_real`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e632029",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2cca6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9d1f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes.get('title').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15b2c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_real_episode = np.array([])\n",
    "for i in range(1000):\n",
    "    create_title = generate_title()\n",
    "    if create_title in episodes.get('title').to_numpy():\n",
    "        is_real_episode = np.append(is_real_episode, create_title)\n",
    "\n",
    "#is_real_episode\n",
    "prob_real = len(np.unique(is_real_episode))/len(episodes.get('title').to_numpy())\n",
    "prob_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c61bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(episodes.get('title').to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587f2437",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(is_real_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2582debe",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(is_real_episode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55802f2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6_7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5038b579",
   "metadata": {},
   "source": [
    "You should find that there's a fairly high probability of generating a title that already exists. This happens because there aren't really that many words in the set of *Friends* episode titles. For example, if we randomly generate the word `'East'`, it's going to have to be followed up with `'German Laundry Detergent'` because the word `'East'` only appears once, which is in the episode title `'The One with the East German Laundry Detergent'`. With a relatively small body of text in *Friends* episode titles, there are many words that can only be followed by one thing.  For simplicity, we'll ignore the fact that we are generating many existing episode titles, though we could significantly improve the situation by basing our titles not just on past *Friends* episode titles, but on a larger body of text, such as the full scripts of all *Friends* episodes. This improvement would come at the cost of much more computation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef00ed3",
   "metadata": {},
   "source": [
    "Now, let's put all our hard work to use and generate episode titles for the first season of the reboot. Most seasons of *Friends* had 24 episodes, so you'll make sure your reboot starts out with a season of 24 episodes. Run the cell below to generate the titles for the first season of the reboot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dead8248",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(1, 25):\n",
    "    print('Reboot Episode', i, ':', generate_title() )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc06c79",
   "metadata": {},
   "source": [
    "In this section, you learned how to use pairs of words to generate text. More generally, sequences of $n$ words (called $n$-grams) can be used to generate text in a similar way. For example, if $n=3$, we would base each next word on the conditional probability of that word appearing after the previous two words. To start, we'd consider what words follow the phrase `'The One'` and how frequently. Perhaps we'd randomly choose the next word to be `'Where'`. We'd choose our next word based on words that follow `'One Where'`, and so on. \n",
    "\n",
    "$n$-grams are used to capture the patterns and sequences of words in a language, which can be useful for a variety of natural language processing tasks, such as language modeling, text generation, machine translation, and more.  While this is a simplified example, modern language models like GPT (Generative Pre-trained Transformer) use more advanced neural network architectures and techniques to capture long-range dependencies between words and generate more coherent and contextual text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dcea4e",
   "metadata": {},
   "source": [
    "<a id='section7'></a>\n",
    "## Section 7: The Last One üîöü•≥\n",
    "\n",
    "<small>(<a href=\"#outline\">return to the outline</a>)</small>\n",
    "\n",
    "In this section, you'll determine how many episodes your reboot should have to be successful. The first question is how should you measure success, in terms of views or in terms of ratings? You suspect that if an episode is good, it‚Äôs only natural that it will get more views *and* higher ratings, right? So there should be a positive correlation between viewership and ratings. You decide to explore the connection between these two variables using regression.\n",
    "\n",
    "Let's start by visualizing the data with a scatter plot to see if linear regression would make sense for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f732bc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "episodes.plot(kind='scatter', x='imdb_rating', y='us_views_millions');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d6bcca",
   "metadata": {},
   "source": [
    "Based on the scatter plot, it seems like there is a slight positive linear association, and so linear regression would be an appropriate tool. Let's continue!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225f2382",
   "metadata": {},
   "source": [
    "**Question 7.1.** The first step will be to convert our data into standard units. Complete the function `standard_units` which takes in an array or Series of numerical data and returns an array with the values in standard units. Then use your function to standardize the `'imdb_rating'` and `'us_views_millions'` columns from `episodes`. Store the standardized arrays in the variables `rating_standard` and `views_standard`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79271751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_units(sequence):\n",
    "    '''Returns the input sequence as an array in standard units.'''\n",
    "    # Convert the input to an array, if it is not already.\n",
    "    sequence = np.array(sequence)\n",
    "    #standard_units_array = np.array([])\n",
    "    #for value in sequence:\n",
    "    standard_units_array = (sequence - sequence.mean())/np.std(sequence) #np.append(standard_units_array, (sequence - sequence.mean())/np.std(sequence))\n",
    "    return standard_units_array\n",
    "\n",
    "rating_standard = standard_units(episodes.get('imdb_rating'))\n",
    "views_standard = standard_units(episodes.get('us_views_millions'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca7b0d4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q7_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ca1113",
   "metadata": {},
   "source": [
    "**Question 7.2.** Now that we know how to convert variables to standard units, we can calculate the correlation. Complete the function `correlation`, which should take in:\n",
    "1. `df`, a DataFrame,\n",
    "2. `independent`, the column label of the independent ($x$) variable, as a string, and \n",
    "3. `dependent`, the column label of the dependent ($y$) variable, as a string.\n",
    "\n",
    "The function should output the correlation between the two variables. \n",
    "\n",
    "Then, use your function to compute the correlation between `'imdb_rating'` and `'us_views_millions'` and store your result in the variable `corr`. Check that the number you get looks reasonable based on the scatter plot above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f00c721",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_standard_columns = episodes.get(['imdb_rating', 'us_views_millions'])\n",
    "get_standard_columns = get_standard_columns.assign(stand_unit_rating = rating_standard)\n",
    "get_standard_columns = get_standard_columns.assign(stand_unit_views = views_standard)\n",
    "get_standard_columns = get_standard_columns.assign(stand_unit_product = rating_standard*views_standard)\n",
    "get_standard_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49850dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(df, independent, dependent):\n",
    "    '''Returns the correlation between the independent and dependent variables in the given DataFrame.'''\n",
    "    df = df.assign(independent_stand_unit = standard_units(df.get(independent)))\n",
    "    df = df.assign(dependent_stand_unit = standard_units(df.get(dependent)))\n",
    "    df = df.assign(stand_unit_product = df.get('independent_stand_unit')*df.get('dependent_stand_unit'))\n",
    "    correlation = df.get('stand_unit_product').mean()\n",
    "    return correlation\n",
    "\n",
    "corr = correlation(episodes, 'imdb_rating', 'us_views_millions')\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944f0606",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q7_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feb2a4f",
   "metadata": {},
   "source": [
    "**Question 7.3.** Now construct two functions, `reg_slope` and `reg_intercept`, which each take in the same three inputs as `correlation`. `reg_slope` should return the slope of the regression line and `reg_intercept` should return the intercept of the regression line, in original units. \n",
    "\n",
    "Use your function to store the slope and intercept of the regression line for  `'imdb_rating'` and `'us_views_millions'` in the variables `slope` and `intercept`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4172fddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_slope(df, independent, dependent):\n",
    "    '''Returns the slope of the regression line in original units.'''\n",
    "    r = correlation(df, independent, dependent)\n",
    "    return r*np.std(df.get(dependent))/np.std(df.get(independent))\n",
    "\n",
    "def reg_intercept(df, independent, dependent):\n",
    "    '''Returns the intercept of the regression line in original units.'''\n",
    "    return df.get(dependent).mean() - reg_slope(df, independent, dependent)*df.get(independent).mean()\n",
    "\n",
    "slope = reg_slope(episodes, 'imdb_rating', 'us_views_millions')\n",
    "intercept = reg_intercept(episodes, 'imdb_rating', 'us_views_millions')\n",
    "slope, intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd117b1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q7_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c51dfc9",
   "metadata": {},
   "source": [
    "**Question 7.4.** Create a function called `predict` that takes in the same three inputs as `correlation`. `predict` should return an array of predicted values of the dependent variable calculated from the regression line. \n",
    "\n",
    "Use your function to create an array of the predicted number of views, in millions, for each episode in the `episodes` DataFrame, based on the episode's rating. Save your answer as `predicted_views`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85d77a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict(df, independent, dependent):\n",
    "    '''Returns an array of predicted values of the dependent variable calculated from the regression line.'''\n",
    "    predicted_array = np.array([])\n",
    "    predicted_views_by_rating = reg_slope(df, independent, dependent)*df.get(independent) + reg_intercept(df, independent, dependent)\n",
    "    predicted_array = np.append(predicted_array, predicted_views_by_rating)\n",
    "    return predicted_array\n",
    "    \n",
    "predicted_views = predict(episodes, 'imdb_rating', 'us_views_millions')\n",
    "predicted_views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7c3ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predicted_views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d82271",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q7_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9366c188",
   "metadata": {},
   "source": [
    "**Question 7.5.** Use the strategy for overlaying scatter plots described in [this tutorial](https://www.statology.org/pandas-scatter-plot-multiple-columns/) to create an overlaid scatter plot with:\n",
    "- a blue dot üîµ for each episode showing the ratings on the $x$-axis and the views on the $y$-axis (as in the scatter plot at the beginning of this section), and\n",
    "- a red dot üî¥ for each episode showing the ratings on the $x$-axis and the **predicted** views on the $y$-axis.\n",
    "\n",
    "The red dots should form a straight line - that's the regression line!\n",
    "\n",
    "***Note:*** This is the first time you've been asked to make an overlaid scatter plot, so you'll need to learn something new to answer this question. Read the linked tutorial carefully and try to mimic their example; you can learn how to do a lot of things this way!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672f5194",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q7_5\n",
    "points: 1\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704b4b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_and_predicted_views = episodes.get(['imdb_rating', 'us_views_millions'])#bpd.DataFrame()\n",
    "actual_and_predicted_views = actual_and_predicted_views.assign(predicted_us_views_millions = predicted_views)\n",
    "actual_and_predicted_views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaa7fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your overlaid scatter plot here.\n",
    "ax1=actual_and_predicted_views.plot(kind='scatter', x='imdb_rating', y='us_views_millions', color='b', label='actual values')\n",
    "\n",
    "#add scatter plot on same graph using B_assists vs. B_points\n",
    "ax2=actual_and_predicted_views.plot(kind='scatter', x='imdb_rating', y='predicted_us_views_millions', color='r', label='predicted values', ax=ax1)\n",
    "\n",
    "#specify x-axis and y-axis labels\n",
    "ax1.set_xlabel('IMDB Ratings')\n",
    "ax1.set_ylabel('Views')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4807446c",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "**Question 7.6.** Use the equation of the regression line to answer the following questions. Check that your answers are reasonable using the scatter plot above.\n",
    "\n",
    "1. What is the predicted number of views, in millions, for an episode with an IMDb rating of 8.6? Save your answer as `question_1`.\n",
    "2. An episode is predicted to have 30 million US views. What is its IMDb rating? Save your answer as `question_2`.\n",
    "3. Suppose an episode in season one has an IMDb rating of $d$ and an episode in season two has a IMDb rating of $d + 2$. How many millions more views would we predict the episode in season two to have compared to the episode in season one? Save your answer as `question_3`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb1ec8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### predicted_views = slope*rating + intercept\n",
    "\n",
    "answer1 = slope*8.6 + intercept\n",
    "answer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a539ce16",
   "metadata": {},
   "outputs": [],
   "source": [
    "### predicted_views = slope*rating + intercept\n",
    "### rating = (predicted_views - intercept)/slope\n",
    "\n",
    "answer2 = (30 - intercept)/slope\n",
    "answer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b85710",
   "metadata": {},
   "outputs": [],
   "source": [
    "### predicted_views_s1 = slope*d + intercept\n",
    "### predicted_views_s2 = slope*(d+2) + intercept = slope*d + slope*2 + intercept\n",
    "### predicted_views_s2 - predicted_views_s1 = slope*2\n",
    "\n",
    "answer3 = slope*2\n",
    "answer3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69785c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_1 = answer1\n",
    "question_2 = answer2\n",
    "question_3 = answer3\n",
    "print('Answer to question 1: ', question_1)\n",
    "print('Answer to question 2: ', question_2)\n",
    "print('Answer to question 3: ', question_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb7d69f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q7_6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e19a20",
   "metadata": {},
   "source": [
    "**Question 7.7.** Now that we have general code to calculate the regression line between any pair of variables in any DataFrame, let's generalize our code for the overlaid scatter plot so we can visualize relationships between other pairs of variables.\n",
    "\n",
    "Complete the function `display_predictions` below. This function should take in the same three inputs as the `correlation` function, create an overlaid scatter plot similar to the one in Question 7.5, and return a string describing the correlation between the variables and the slope and intercept of the regression line.\n",
    "\n",
    "Then, use your function to create a scatter plot and calculate the regression line that would allow you to predict IMDb rating based on viewership in millions. Store the result of your function call in the variable `rating_prediction`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3b5bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_values = predict(episodes, 'imdb_rating', 'us_views_millions')\n",
    "test_df = episodes.assign(predictions = predict_values).get('imdb_rating')\n",
    "test_df#.get('imdb_rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53423201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_predictions(df, independent, dependent):\n",
    "    '''Generates an overlaid scatter plot showing the relationship between the independent and dependent variables in df.\n",
    "    Returns a string describing the correlation and the slope and intercept of the regression line.'''\n",
    "    # Create your overlaid scatter plot here.\n",
    "    ax1=df.plot(kind='scatter', x=independent, y=dependent, color='b', label='actual values')\n",
    "\n",
    "    df = df.assign(prediction = predict(df, independent, dependent))\n",
    "    #add scatter plot on same graph using B_assists vs. B_points\n",
    "    ax2=df.plot(kind='scatter', x=independent, y='prediction', color='r', label='predicted values', ax=ax1)\n",
    "\n",
    "    #specify x-axis and y-axis labels\n",
    "    ax1.set_xlabel(independent.title())\n",
    "    ax1.set_ylabel(dependent.title())\n",
    "    \n",
    "    # We've provided the code for the return statement.\n",
    "    return ('The correlation between {0} and {1} is {2}. ' +\\\n",
    "           ' The slope of the regression line is {3}.' + \\\n",
    "           ' The intercept of the regression line is {4}.')\\\n",
    "                .format(independent,\n",
    "                        dependent,\n",
    "                        str(round(correlation(df, independent, dependent), 2)),\n",
    "                        str(round(reg_slope(df, independent, dependent), 2)),\n",
    "                        str(round(reg_intercept(df, independent, dependent), 2)))\n",
    "\n",
    "# Your function should produce the same scatter plot as in Question 7.5 on the inputs below.\n",
    "# Make sure to test it out on other inputs too.\n",
    "display_predictions(episodes, 'imdb_rating', 'us_views_millions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1645a0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_predictions(get_standard_columns, 'stand_unit_rating', 'stand_unit_views')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6c8a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_prediction = display_predictions(episodes, 'us_views_millions', 'imdb_rating')\n",
    "rating_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895f2987",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q7_7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebb7424",
   "metadata": {},
   "source": [
    "**Question 7.8.** There seem to be just a few episodes with 40 million views or more. Since outliers can have a big impact on the regression line, let's see how different our regression line would look if we eliminated these outliers. Again using the `display_predictions` function you just wrote, create a scatter plot and calculate the regression line that would allow you to predict IMDb rating based on viewership, but based only on episodes with less than 40 million views. Store the result of your function call in the variable `no_outliers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75df2ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "less_than_40M = episodes[episodes.get('us_views_millions') < 40]\n",
    "less_than_40M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677a9f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_outliers = display_predictions(less_than_40M, 'us_views_millions', 'imdb_rating')\n",
    "no_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4304ab6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q7_8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc6c71b",
   "metadata": {},
   "source": [
    "Let's use some of the regression tools we've developed in this section to answer an important question about the reboot: how many episodes should it have? You are about to meet with the producers of the reboot to discuss this. To inform your discussion, you want to see how viewership of *Friends* changed as more episodes were made. \n",
    "\n",
    "**Question 7.9.** Call your `display_predictions` function so that it displays the regression line predicting `'us_views_millions'` based on the number of *Friends* episodes produced.\n",
    "\n",
    "***Hint:*** The `episodes` DataFrame does not have a column that counts the episode number from 1 to the total number of episodes. You'll need to add one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eae2113",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_episode_df = episodes.assign(ep_counter = episodes.index+1)\n",
    "count_episode_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5382200",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q7_9\n",
    "points: 1\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d8ff96",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_predictions(count_episode_df, 'ep_counter', 'us_views_millions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077628e2",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "Since the slope of the regression line is negative, this means that viewership of *Friends* ultimately declined as more episodes were created. But this wasn't always the case. When the show was just starting off, new episodes were getting more views than past episodes. The regression line fit to just the first 30 episodes, for example, would have a strongly positive slope. \n",
    "\n",
    "You interpret this data as meaning that maybe *Friends* went on for too long. You want your reboot to have few enough episodes that the excitement of the show doesn't wear off.\n",
    "\n",
    "**Question 7.10.**  Use your `display_predictions` function to display the regression lines predicting `'us_views_millions'` based on the first 20, 40, 60, 80, 100, and 120 episodes. The largest such number of episodes with a positive slope is the number of episodes you think the reboot should have. Save this number of episodes as `reboot_length`. You can type this number in manually after looking at the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97982cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot your regression lines here.\n",
    "up_to_20 = display_predictions(count_episode_df[count_episode_df.get('ep_counter')<=20], 'ep_counter', 'us_views_millions')\n",
    "up_to_40 = display_predictions(count_episode_df[count_episode_df.get('ep_counter')<=40], 'ep_counter', 'us_views_millions')\n",
    "up_to_60 = display_predictions(count_episode_df[count_episode_df.get('ep_counter')<=60], 'ep_counter', 'us_views_millions')\n",
    "up_to_80 = display_predictions(count_episode_df[count_episode_df.get('ep_counter')<=80], 'ep_counter', 'us_views_millions')\n",
    "up_to_100 = display_predictions(count_episode_df[count_episode_df.get('ep_counter')<=100], 'ep_counter', 'us_views_millions')\n",
    "up_to_120 = display_predictions(count_episode_df[count_episode_df.get('ep_counter')<=120], 'ep_counter', 'us_views_millions')\n",
    "\n",
    "up_to_20, up_to_40, up_to_60, up_to_80, up_to_100, up_to_120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3045327",
   "metadata": {},
   "outputs": [],
   "source": [
    "reboot_length = 80\n",
    "reboot_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7191fc11",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q7_10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50368200",
   "metadata": {},
   "source": [
    "Now that you know how many episodes your reboot should have, you've completed your plans for the reboot. Well done and best wishes for a successful show!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b609eea",
   "metadata": {},
   "source": [
    "## Finish Line üèÅ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d3d7bb",
   "metadata": {},
   "source": [
    "### **Congratulations! üéâ You've completed the Final Project!**\n",
    "\n",
    "**_Citations:_** Did you use any generative artificial intelligence tools to assist you on this assignment? If so, please state, for each tool you used, the name of the tool (ex. ChatGPT) and the problem(s) in this assignment where you used the tool for help."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcc9bbd",
   "metadata": {},
   "source": [
    "<hr style=\"color:Maroon;background-color:Maroon;border:0 none; height: 3px;\">\n",
    "\n",
    "Please cite tools here.\n",
    "\n",
    "<hr style=\"color:Maroon;background-color:Maroon;border:0 none; height: 3px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65ce962",
   "metadata": {},
   "source": [
    "To submit your assignment:\n",
    "\n",
    "1. Select `Kernel -> Restart & Run All` to ensure that you have executed all cells, including the test cells. <p style=\"color: red\"><b>‚ö†Ô∏è Important!</b> We will allot 20 minutes of computer time to run your notebook. If your notebook takes longer than this to run, it may not pass the autograder! Select \"Kernel -> Restart and Run All\" to time how long your notebook takes. A notebook with correct answers should take less than 10 minutes.</p>\n",
    "\n",
    "2. Read through the notebook to make sure everything is fine and all tests passed.\n",
    "3. Run the cell below to run all tests, and make sure that they all pass.\n",
    "4. Download your notebook using File -> Download as -> Notebook (.ipynb), then upload your notebook to Gradescope. Don't forget to add your partner to your group on Gradescope!\n",
    "5. Stick around while the Gradescope autograder grades your work. Make sure you see that all tests have passed on Gradescope.\n",
    "6. Check that you have a confirmation email from Gradescope and save it as proof of your submission. \n",
    "\n",
    "If running all the tests at once causes a test to fail that didn't fail when you ran the notebook in order, check to see if you changed a variable's value later in your code. Make sure to use new variable names instead of reusing ones that are used in the tests. \n",
    "\n",
    "Remember, the tests here and on Gradescope just check the format of your answers. We will run correctness tests after the assignment's due date has passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3feaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93560960",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"images/friends_anticipation.png\" width=\"600\" height=\"600\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
